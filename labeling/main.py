import os
import json
import time
import logging
from pathlib import Path
from typing import Any, Dict, List, Tuple

from tqdm import tqdm, trange
from dotenv import load_dotenv
from datasets import load_dataset, Dataset
from huggingface_hub import HfApi, create_repo

import google.genai as genai
from moviepy import VideoFileClip, concatenate_videoclips

from segment_processor import generate_segment_queries
from episode_processor import generate_episode_queries
from series_processor import generate_series_queries

# ================== åŸºæœ¬è¨­å®š ==================
logging.basicConfig(level=logging.INFO)
load_dotenv()

# æ”¯æ´å–®å€‹æˆ–å¤šå€‹ API Keyï¼ˆç”¨é€—è™Ÿåˆ†éš”æˆ– JSON é™£åˆ—ï¼‰
GEMINI_API_KEYS = [k.strip() for k in os.getenv("GEMINI_API_KEY", "").strip().split(',') if k.strip()]

HF_TOKEN = os.getenv("HF_TOKEN", "")

if not GEMINI_API_KEYS:
    raise RuntimeError("è«‹å…ˆè¨­å®š GEMINI_API_KEY")
if not HF_TOKEN:
    raise RuntimeError("è«‹å…ˆè¨­å®š HF_TOKEN")

# API Key è¼ªæ›
current_key_index = 0

def get_next_api_key():
    """è¼ªæ›ä½¿ç”¨ API Key ä»¥é¿å…é€Ÿç‡é™åˆ¶"""
    global current_key_index
    key = GEMINI_API_KEYS[current_key_index]
    current_key_index = (current_key_index + 1) % len(GEMINI_API_KEYS)
    return key

# Hugging Face å€‰åº«è¨­å®š
HF_REPO_SEGMENT = "TakalaWang/anime-2024-winter-segment-queries"
HF_REPO_EPISODE = "TakalaWang/anime-2024-winter-episode-queries"
HF_REPO_SERIES = "TakalaWang/anime-2024-winter-series-queries"

# å¿«å–ç›®éŒ„
CACHE_DIR = Path("./cache_gemini_video")
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# è³‡æ–™é›†è¨­å®š
TEST_DATASET = "JacobLinCool/anime-2024"
TEST_SPLIT = "winter"
SEGMENT_LENGTH = 60  # ç‰‡æ®µé•·åº¦ï¼ˆç§’ï¼‰
SEGMENT_OVERLAP = 5  # ç‰‡æ®µé‡ç–Šï¼ˆç§’ï¼‰

def get_client():
    """ç²å– Gemini clientï¼ˆä½¿ç”¨è¼ªæ›çš„ API keyï¼‰"""
    return genai.Client(api_key=get_next_api_key())


# ================== Hugging Face å·¥å…·å‡½æ•¸ ==================
def ensure_hf_repos():
    """ç¢ºä¿ Hugging Face å€‰åº«å­˜åœ¨"""
    create_repo(HF_REPO_SEGMENT, token=HF_TOKEN, repo_type="dataset", exist_ok=True)
    create_repo(HF_REPO_EPISODE, token=HF_TOKEN, repo_type="dataset", exist_ok=True)
    create_repo(HF_REPO_SERIES, token=HF_TOKEN, repo_type="dataset", exist_ok=True)


def upload_json_to_hf(repo_id: str, path: Path, repo_path: str):
    """ä¸Šå‚³ JSON æª”æ¡ˆåˆ° Hugging Face"""
    api = HfApi(token=HF_TOKEN)
    api.upload_file(
        path_or_fileobj=str(path),
        repo_id=repo_id,
        path_in_repo=repo_path,
        repo_type="dataset",
    )


def upload_dataset_to_hf(repo_id: str, data: List[Dict[str, Any]]):
    """ä¸Šå‚³æ•¸æ“šåˆ—è¡¨åˆ° HF dataset æ ¼å¼ï¼Œå•Ÿç”¨ data viewer"""

    # å‰µå»º Dataset å°è±¡ï¼ˆç›´æ¥å¾å­—å…¸åˆ—è¡¨ï¼Œä¿ç•™åµŒå¥—çµæ§‹ï¼‰
    dataset = Dataset.from_list(data)

    # ä¸Šå‚³åˆ° HF
    dataset.push_to_hub(repo_id, token=HF_TOKEN)

    print(f"ğŸ“Š å·²ä¸Šå‚³ dataset åˆ° HF: {repo_id} ({len(data)} ç­†è¨˜éŒ„)")


# ================== æ•¸æ“šé›†ç®¡ç† ==================
def create_metadata_jsonl(repo_id: str, metadata_list: List[Dict[str, Any]], metadata_filename: str = "metadata.jsonl"):
    """å‰µå»º metadata.jsonl æ–‡ä»¶ä¸¦ä¸Šå‚³åˆ° HF datasetï¼Œå•Ÿç”¨ data viewer"""
    # ç¢ºä¿æ‰€æœ‰è¨˜éŒ„éƒ½æœ‰ file_name å­—æ®µ
    processed_metadata = []
    for item in metadata_list:
        if "file_name" not in item:
            # å¦‚æœæ²’æœ‰ file_nameï¼Œå˜—è©¦å¾å…¶ä»–å­—æ®µæ¨æ–·
            if "episode_name" in item:
                item["file_name"] = f"videos/{item.get('series_name', 'unknown')}/episode_{item['episode_name']}.mp4"
            elif "segment_index" in item:
                item["file_name"] = f"videos/segment_{item.get('episode_id', 'unknown')}_seg{item['segment_index']}.mp4"
            else:
                continue  # è·³éæ²’æœ‰æ–‡ä»¶åçš„è¨˜éŒ„

        processed_metadata.append(item)

    # å¯«å…¥æœ¬åœ° metadata.jsonl æ–‡ä»¶
    metadata_path = CACHE_DIR / metadata_filename
    with open(metadata_path, "w", encoding="utf-8") as f:
        for item in processed_metadata:
            json.dump(item, f, ensure_ascii=False)
            f.write("\n")

    # ä¸Šå‚³åˆ° HF
    api = HfApi(token=HF_TOKEN)
    api.upload_file(
        path_or_fileobj=str(metadata_path),
        repo_id=repo_id,
        path_in_repo=metadata_filename,
        repo_type="dataset",
    )

    print(f"ğŸ“‹ å·²å‰µå»ºä¸¦ä¸Šå‚³ metadata.jsonl åˆ° {repo_id} ({len(processed_metadata)} ç­†è¨˜éŒ„)")
    return processed_metadata


def upload_video_to_hf(repo_id: str, video_path: Path, repo_path: str):
    """ä¸Šå‚³å½±ç‰‡æª”æ¡ˆåˆ° HuggingFace"""
    api = HfApi(token=HF_TOKEN)
    api.upload_file(
        path_or_fileobj=str(video_path),
        repo_id=repo_id,
        path_in_repo=repo_path,
        repo_type="dataset",
    )
    print(f"ğŸ“¤ å·²ä¸Šå‚³å½±ç‰‡åˆ° HF: {repo_path}")


# ================== å½±ç‰‡è™•ç†å·¥å…·å‡½æ•¸ ==================
def extract_video_segment(video_path: str, start_s: float, end_s: float, output_path: Path):
    """åˆ‡å‰²å½±ç‰‡ç‰‡æ®µ"""
    with VideoFileClip(video_path) as video:
        segment = video.subclipped(start_s, end_s)
        segment.write_videofile(
            str(output_path),
            codec="libx264",
            audio_codec="aac",
            temp_audiofile=str(output_path.parent / f"temp_{output_path.stem}_audio.m4a"),
            remove_temp=True,
            logger=None,
        )


def concatenate_videos(video_paths: List[str], output_path: Path):
    """åˆä½µå¤šå€‹å½±ç‰‡"""
    clips = []
    for path in video_paths:
        clips.append(VideoFileClip(path))
    
    final_clip = concatenate_videoclips(clips, method="compose")
    final_clip.write_videofile(
        str(output_path),
        codec="libx264",
        audio_codec="aac",
        temp_audiofile=str(output_path.parent / f"temp_{output_path.stem}_audio.m4a"),
        remove_temp=True,
        logger=None,
    )
    
    # é—œé–‰æ‰€æœ‰ clips
    for clip in clips:
        clip.close()
    final_clip.close()
    
    print(f"ğŸ”— å·²åˆä½µå½±ç‰‡: {output_path.name}")


def get_video_duration_from_path(path: str) -> float:
    """ç²å–å½±ç‰‡é•·åº¦ï¼ˆç§’ï¼‰"""
    with VideoFileClip(path) as clip:
        return clip.duration


# ================== API å·¥å…·å‡½æ•¸ ==================
def call_with_retry(fn, *args, **kwargs):
    """å¸¶é‡è©¦æ©Ÿåˆ¶çš„ API å‘¼å«"""
    sleep_sec = kwargs.pop("sleep_sec", 5)
    while True:
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            msg = str(e)
            if "429" in msg or "rate" in msg.lower():
                print(f"[rate limited] sleep {sleep_sec}s and retry ...")
                time.sleep(sleep_sec)
                continue
            print(f"[error] {e} -> sleep {sleep_sec}s and retry ...")
            time.sleep(sleep_sec)  
        


# ================== è³‡æ–™è™•ç†å‡½æ•¸ ==================
def load_and_group_dataset() -> Dict[str, List[Dict[str, Any]]]:
    """è¼‰å…¥è³‡æ–™é›†ä¸¦æŒ‰ç³»åˆ—åˆ†çµ„"""
    print("è¼‰å…¥è³‡æ–™é›†...")
    ds = load_dataset(TEST_DATASET, TEST_SPLIT, split="train")
    ds_raw = ds.with_format("arrow")

    # ä¾ series åˆ†çµ„
    series_groups: Dict[str, List[Dict[str, Any]]] = {}
    for i in trange(len(ds_raw), desc="group by series"):
        row = ds_raw[i:i+1]
        series_name = row["series_name"][0].as_py()
        episode_name = row["episode_name"][0].as_py()
        video_path = row["video"][0]["path"].as_py()
        duration = row["duration"][0].as_py() if "duration" in row.column_names else None
        release_date = row["release_date"][0].as_py() if "release_date" in row.column_names else None

        series_groups.setdefault(series_name, []).append({
            "episode_name": episode_name,
            "series_name": series_name,
            "video_path": video_path,
            "duration": duration,
            "release_date": release_date,
        })

    return series_groups


def process_segments_for_episode(episode_id: str, video_path: str, duration_s: float) -> List[Dict[str, Any]]:
    """è™•ç†å–®é›†çš„ç‰‡æ®µç´šåˆ¥æŸ¥è©¢ç”Ÿæˆ"""
    segment_ranges = []
    start = 0.0
    while start < duration_s:
        end = min(start + SEGMENT_LENGTH, duration_s)
        if end - start < 5:
            break
        segment_ranges.append((start, end))
        if end >= duration_s:
            break
        start = start + SEGMENT_LENGTH - SEGMENT_OVERLAP

    segment_files = []
    for seg_idx, (s, e) in enumerate(tqdm(segment_ranges, desc=f"åˆ‡å‰²ç‰‡æ®µ {episode_id}", unit="seg")):
        segment_video_path = CACHE_DIR / f"segment_{episode_id}_seg{seg_idx}.mp4"
        if not segment_video_path.exists():
            extract_video_segment(video_path, s, e, segment_video_path)

        segment_files.append({
            "index": seg_idx,
            "path": segment_video_path,
        })

    # è™•ç†æ¯å€‹ç‰‡æ®µ
    seg_results = []
    for seg_info in segment_files:
        seg_idx = seg_info["index"]
        segment_path = seg_info["path"]

        cache_path = CACHE_DIR / f"segment_{episode_id}_seg{seg_idx}.json"
        if cache_path.exists():
            with open(cache_path, "r", encoding="utf-8") as f:
                cached = json.load(f)
                seg_results.append(cached)
                continue

        client = get_client()

        data = call_with_retry(
            generate_segment_queries,
            client=client,
            video_path=str(segment_path),
            sleep_sec=5,
        )

        record = {
            "series_name": "",  # ç¨å¾Œå¡«å……
            "episode_id": episode_id,
            "segment_index": seg_idx,
            "release_date": None,  # ç¨å¾Œå¡«å……
            "queries": data,
        }

        # å„²å­˜å¿«å–
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(record, f, ensure_ascii=False, indent=2)

        seg_results.append(record)

        # ä¸Šå‚³ç‰‡æ®µå½±ç‰‡åˆ° HF
        segment_hf_path = f"videos/segment_{episode_id}_seg{seg_idx}.mp4"
        print(f"ä¸Šå‚³ç‰‡æ®µ {seg_idx} åˆ° HuggingFace...")
        upload_video_to_hf(
            HF_REPO_SEGMENT,
            segment_path,
            segment_hf_path
        )

        # æ·»åŠ  file_name å­—æ®µåˆ° record
        record["file_name"] = segment_hf_path

    return seg_results


def process_episode_level(series_name: str, episode_id: str, video_path: str, duration_s: float, release_date: Any) -> Dict[str, Any]:
    """è™•ç†é›†æ•¸ç´šåˆ¥çš„æŸ¥è©¢ç”Ÿæˆ"""
    client = get_client()

    uploaded = client.files.upload(file=video_path)
    file_uri = uploaded.uri


    epi_result = call_with_retry(
        generate_episode_queries,
        client=client,
        file_uri=file_uri,
        sleep_sec=5,
    )

    # ä¸Šå‚³å®Œæ•´é›†æ•¸å½±ç‰‡åˆ° HF
    episode_video_hf_path = f"videos/{series_name}/episode_{episode_id}.mp4"
    print("  ğŸ“¤ ä¸Šå‚³å®Œæ•´é›†æ•¸åˆ° HuggingFace...")
    upload_video_to_hf(HF_REPO_EPISODE, Path(video_path), episode_video_hf_path)

    # åŒ…è£ episode metadata èˆ‡æ¨¡å‹å›æ‡‰
    episode_record = {
        "file_name": episode_video_hf_path,  # æ·»åŠ  file_name å­—æ®µç”¨æ–¼ data viewer
        "series_name": series_name,
        "episode_name": episode_id,
        "release_date": release_date,
        "duration": duration_s,
        "model_response": epi_result,
    }

    # ä»ç„¶ä¿å­˜å–®å€‹ JSON æ–‡ä»¶ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
    epi_local = CACHE_DIR / f"episode_{episode_id}.json"
    with open(epi_local, "w", encoding="utf-8") as f:
        json.dump(episode_record, f, ensure_ascii=False, indent=2)
    upload_json_to_hf(HF_REPO_EPISODE, epi_local, f"episode_{episode_id}.json")

    return episode_record


def process_single_episode(series_name: str, episode_info: Dict[str, Any]) -> Tuple[str, str, float, Any, Dict[str, Any]]:
    """è™•ç†å–®å€‹é›†æ•¸çš„æ‰€æœ‰å±¤ç´šï¼Œè¿”å› (episode_id, video_path, duration, release_date, episode_record)"""
    episode_id = episode_info["episode_name"]
    video_path = episode_info["video_path"]

    duration_s = get_video_duration_from_path(video_path)

    print(f"\n{'='*60}")
    print(f"è™•ç†é›†æ•¸: {episode_id}")
    print(f"å½±ç‰‡é•·åº¦: {duration_s:.2f} ç§’")
    print(f"{'='*60}\n")

    # ===== Segment level =====
    seg_results = process_segments_for_episode(episode_id, video_path, duration_s)

    # å¡«å…… series_name å’Œ release_date
    for seg in seg_results:
        seg["series_name"] = series_name
        seg["release_date"] = episode_info.get("release_date")

    # ä¸Šå‚³ segment JSON å½™ç¸½
    seg_local = CACHE_DIR / f"segment_{episode_id}.json"
    with open(seg_local, "w", encoding="utf-8") as f:
        json.dump(seg_results, f, ensure_ascii=False, indent=2)
    upload_json_to_hf(HF_REPO_SEGMENT, seg_local, f"segment_{episode_id}.json")

    # ===== Episode level =====
    episode_record = process_episode_level(series_name, episode_id, video_path, duration_s, episode_info.get("release_date"))

    return episode_id, video_path, duration_s, episode_info.get("release_date"), episode_record


def process_series_level(series_name: str, processed_episodes: List[Tuple[str, str, float, Any]]) -> Dict[str, Any]:
    """è™•ç†ç³»åˆ—ç´šåˆ¥çš„æŸ¥è©¢ç”Ÿæˆ"""
    print(f"\nğŸ“ æ­¥é©Ÿ 4: è™•ç†æ•´å­£è³‡æ–™: {series_name} ...")

    client = get_client()

    # ç‚º series æ§‹å»ºç°¡å–®çš„æ–‡å­—æè¿°ï¼ˆç›®å‰å¯æ ¹æ“šéœ€è¦æ“´å……ï¼‰
    series_text_lines = [f"Series: {series_name}"]
    for ep_id, vp, dur, rd in processed_episodes:
        series_text_lines.append(f"Episode: {ep_id} | duration: {dur:.2f} s | path: {vp} | release_date: {rd}")
    series_text = "\n".join(series_text_lines)

    series_result = call_with_retry(
        generate_series_queries,
        client=client,
        series_text=series_text,
        sleep_sec=5,
    )

    # åˆä½µä¸¦ä¸Šå‚³æ•´å­£å½±ç‰‡
    print("  æº–å‚™æ•´å­£å½±ç‰‡...")
    episode_video_paths = [vp for _, vp, _, _ in processed_episodes]
    series_video_path = CACHE_DIR / f"series_{series_name}.mp4"
    if not series_video_path.exists():
        print("  ğŸ”— é–‹å§‹åˆä½µæ•´å­£å½±ç‰‡...")
        concatenate_videos(episode_video_paths, series_video_path)

    print("  ğŸ“¤ ä¸Šå‚³æ•´å­£å½±ç‰‡åˆ° HuggingFace...")
    upload_video_to_hf(
        HF_REPO_SERIES,
        series_video_path,
        f"videos/series_{series_name}.mp4"
    )

    # å»ºç«‹ series metadataï¼ˆä¸è¦å­˜ episode_nameï¼Œåƒ…å­˜ release_dates èˆ‡æ¨¡å‹å›æ‡‰ï¼‰
    release_dates = sorted({rd for (_eid, _vp, _dur, rd) in processed_episodes if rd is not None})
    series_record = {
        "series_name": series_name,
        "episode_count": len(processed_episodes),
        "release_dates": release_dates,
        "model_response": series_result,
    }

    series_local = CACHE_DIR / f"series_{series_name}.json"
    with open(series_local, "w", encoding="utf-8") as f:
        json.dump(series_record, f, ensure_ascii=False, indent=2)
    upload_json_to_hf(HF_REPO_SERIES, series_local, f"series_{series_name}.json")

    return series_record


def process_single_series(series_name: str, episodes: List[Dict[str, Any]]) -> Tuple[Tuple[str, List[Tuple[str, str, float, Any]]], List[Dict[str, Any]], List[Dict[str, Any]]]:
    """è™•ç†å–®å€‹ç³»åˆ—çš„æ‰€æœ‰é›†æ•¸ï¼Œè¿”å› (result, episode_metadata, segment_metadata)"""
    print(f"é–‹å§‹è™•ç†ç³»åˆ—: {series_name} (å…± {len(episodes)} é›†)")

    processed_episodes = []
    episode_metadata = []
    segment_metadata = []
    episode_video_paths = []  # ç”¨æ–¼æœ€å¾Œåˆä½µæ•´å­£

    for episode_info in tqdm(episodes, desc=f"episodes - {series_name}", unit="ep"):
        episode_id, video_path, duration_s, release_date, episode_record = process_single_episode(series_name, episode_info)
        processed_episodes.append((episode_id, video_path, duration_s, release_date))
        episode_metadata.append(episode_record)
        episode_video_paths.append(video_path)

        # æ”¶é›† segment metadata
        seg_results = process_segments_for_episode(episode_id, video_path, duration_s)
        # å¡«å…… series_name å’Œ release_date
        for seg in seg_results:
            seg["series_name"] = series_name
            seg["release_date"] = release_date
        segment_metadata.extend(seg_results)

    # è™•ç†ç³»åˆ—ç´šåˆ¥
    process_series_level(series_name, processed_episodes)

    return (series_name, processed_episodes), episode_metadata, segment_metadata


def process_all_series(series_groups: Dict[str, List[Dict[str, Any]]]) -> Tuple[List[Tuple[str, List[Tuple[str, str, float, Any]]]], List[Dict[str, Any]], List[Dict[str, Any]]]:
    """è™•ç†æ‰€æœ‰ç³»åˆ—ï¼Œè¿”å› (processed_series, all_episode_metadata, all_segment_metadata)"""
    processed_series = []
    all_episode_metadata = []
    all_segment_metadata = []

    for series_name, episodes in series_groups.items():
        result, episode_metadata, segment_metadata = process_single_series(series_name, episodes)
        processed_series.append(result)
        all_episode_metadata.extend(episode_metadata)
        all_segment_metadata.extend(segment_metadata)

    return processed_series, all_episode_metadata, all_segment_metadata


# ================== ä¸»ç¨‹å¼ ==================
def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    ensure_hf_repos()
    
    print(f"å·²è¨­å®š {len(GEMINI_API_KEYS)} å€‹ Gemini API Key")

    # è¼‰å…¥ä¸¦åˆ†çµ„è³‡æ–™é›†
    series_groups = load_and_group_dataset()

    # è™•ç†æ‰€æœ‰ç³»åˆ—èˆ‡ episodes
    processed_series, all_episode_metadata, all_segment_metadata = process_all_series(series_groups)

    # å‰µå»º metadata.jsonl æ–‡ä»¶ä»¥å•Ÿç”¨ Dataset Viewer
    if all_episode_metadata:
        create_metadata_jsonl(HF_REPO_EPISODE, all_episode_metadata, "metadata.jsonl")

    if all_segment_metadata:
        create_metadata_jsonl(HF_REPO_SEGMENT, all_segment_metadata, "metadata.jsonl")

    # å®Œæˆç¸½çµ
    total_series = len(processed_series)
    total_episodes = sum(len(eps) for _, eps in processed_series)

    print("\n" + "="*60)
    print("ğŸ‰ è™•ç†å®Œæˆï¼")
    print(f"âœ… è™•ç†äº† {total_series} å€‹ç³»åˆ—ï¼Œ{total_episodes} é›†")
    print(f"ğŸ“Š Episode metadata: {len(all_episode_metadata)} ç­†")
    print(f"ğŸ“Š Segment metadata: {len(all_segment_metadata)} ç­†")
    print("ğŸ” Dataset Viewer ç¾å·²å•Ÿç”¨ï¼")
    print("="*60)


if __name__ == "__main__":
    main()
