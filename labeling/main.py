import os
import json
import time
import logging
import tempfile
import shutil
import subprocess
import threading
from pathlib import Path
from typing import Any, Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from dotenv import load_dotenv
from datasets import load_dataset, Video
from huggingface_hub import HfApi, create_repo
import google.genai as genai
from moviepy import VideoFileClip

from segment_processor import generate_segment_queries, BlockedContentError
from episode_processor import generate_episode_queries
from series_processor import generate_series_queries
from update_metadata import (
    update_segment_metadata,
    update_episode_metadata,
    update_series_metadata,
)

# ========= åŸºæœ¬è¨­å®š =========
load_dotenv()
logging.basicConfig(level=logging.INFO)

GEMINI_KEYS = [k for k in os.getenv("GEMINI_API_KEY", "").split(",") if k.strip()]
HF_TOKEN = os.getenv("HF_TOKEN", "")
if not GEMINI_KEYS:
    raise RuntimeError("need GEMINI_API_KEY")
if not HF_TOKEN:
    raise RuntimeError("need HF_TOKEN")

HF_SEG = "TakalaWang/anime-2024-winter-segment-queries"
HF_EP  = "TakalaWang/anime-2024-winter-episode-queries"
HF_SER = "TakalaWang/anime-2024-winter-series-queries"

CACHE_ROOT = Path("cache_gemini_video"); CACHE_ROOT.mkdir(exist_ok=True)
VIDEO_ROOT = CACHE_ROOT / "videos"; VIDEO_ROOT.mkdir(exist_ok=True)
ERROR_LOG = CACHE_ROOT / "error_log.jsonl"

DATASET = "JacobLinCool/anime-2024"
SUBSET = "winter"
SEG_LEN = 60
SEG_OVERLAP = 5

# ========= å°å·¥å…· =========
_key_lock = threading.Lock()
_key_idx = 0

def safe_name(s: str) -> str:
    """æŠŠ series åç¨±è®Šæˆæª”åå®‰å…¨çš„å½¢å¼"""
    return s.replace(" ", "_").replace("/", "_").strip()

def make_client() -> genai.Client:
    """
    è¼ªæµæ‹¿ä¸€æŠŠ Gemini key
    ç„¡è«–æˆåŠŸæˆ–å¤±æ•—ï¼Œä½ æ¯æ¬¡å‘¼å«é€™å€‹éƒ½æœƒæ‹¿åˆ°ä¸‹ä¸€æŠŠ
    """
    global _key_idx
    with _key_lock:
        key = GEMINI_KEYS[_key_idx]
        _key_idx = (_key_idx + 1) % len(GEMINI_KEYS)
        print(f"ğŸ”‘ ä½¿ç”¨ Gemini key #{_key_idx}")
    return genai.Client(api_key=key)

def log_error(context: str, error: str):
    ERROR_LOG.parent.mkdir(parents=True, exist_ok=True)
    with ERROR_LOG.open("a", encoding="utf-8") as f:
        json.dump(
            {
                "time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "context": context,
                "error": error,
            },
            f,
            ensure_ascii=False,
        )
        f.write("\n")

# ======== åˆ¤æ–·è¦ä¸è¦é‡è©¦ ========
def _is_retryable_error(e: Exception) -> bool:
    s = str(e)
    retry_keys = [
        "503",  # Service Unavailable
        "429",  # Too Many Requests
        "UNAVAILABLE",
        "DEADLINE_EXCEEDED",
        "temporarily overloaded",
    ]
    return any(k in s for k in retry_keys)

def _is_fatal_error(e: Exception) -> bool:
    s = str(e)
    fatal_keys = [
        "PERMISSION_DENIED",  # 403ï¼Œè¢«åœç”¨
        "CONSUMER_SUSPENDED",
        "INVALID_ARGUMENT",   # 400
        "The request's total referenced files bytes are too large",
    ]
    return any(k in s for k in fatal_keys)

# ======== é€šç”¨é‡è©¦å™¨ï¼šæ¯ä¸€è¼ªéƒ½æ› key ========
def retry(fn_factory, ctx: str, times: int = 5):
    """
    fn_factory: ä¸€å€‹æ¥æ”¶ client çš„å‡½å¼ï¼Œä¾‹å¦‚ lambda c: c.files.upload(...)
    æ¯æ¬¡é‡è©¦éƒ½æœƒé‡æ–°å»ºä¸€å€‹ä½¿ç”¨ä¸‹ä¸€æŠŠ key çš„ client
    æˆåŠŸã€å¤±æ•—éƒ½æœƒã€Œæ¶ˆè€—ã€æ‰ä¸€æŠŠ keyï¼Œé”åˆ°å¹³å‡åˆ†é…
    """
    last = None
    for i in range(times):
        client = make_client()  # é€™è£¡æ˜¯é—œéµï¼šæ¯ä¸€è¼ªéƒ½æ› client/æ› key
        try:
            return fn_factory(client)
        except Exception as e:
            last = e

            if _is_fatal_error(e):
                logging.error(f"{ctx} fatal error: {e}")
                break

            if _is_retryable_error(e):
                # ç¬¬ä¸€æ¬¡ç‚¸å¾—å¾ˆæ­£å¸¸ï¼Œçµ¦çŸ­ä¸€é»
                wait = 2 if i == 0 else min(3 * (2 ** i), 10)
            else:
                wait = 5

            logging.warning(f"{ctx} ç¬¬ {i+1} æ¬¡å¤±æ•—ï¼Œ{wait}s å¾Œæ›ä¸‹ä¸€æŠŠ key å†è©¦ï¼š{e}")
            time.sleep(wait)

    log_error(ctx, str(last))
    return None

# ======== ä¸Šå‚³ ========
def upload_file_to_gemini(path: str) -> Optional[str]:
    """
    ä¸Šå‚³æª”æ¡ˆåˆ° Gemini
    ä¸Šå‚³æœ¬èº«ä¹Ÿé€é retryï¼Œæ‰€ä»¥æ¯æ¬¡æˆåŠŸ/å¤±æ•—éƒ½æœƒè¼ª key
    """
    p = Path(path)
    try:
        p.name.encode("ascii")
        up = str(p)
    except UnicodeEncodeError:
        tmp = Path(tempfile.gettempdir()) / f"tmp_{int(time.time()*1000)}{p.suffix}"
        shutil.copy2(p, tmp)
        up = str(tmp)

    # ç”¨ retryï¼Œè®“å®ƒè‡ªå·±æ› client
    obj = retry(lambda c: c.files.upload(file=up), f"upload {path}")
    if not obj:
        return None

    # ç­‰å¾…è™•ç†å®Œæˆï¼šé€™è£¡ä¹Ÿå¯ä»¥æ› client ä¾† get
    while obj.state.name == "PROCESSING":
        time.sleep(5)
        client = make_client()
        obj = client.files.get(name=obj.name)

    if obj.state.name == "FAILED":
        log_error(f"gemini processing {path}", "state=FAILED")
        return None

    return obj.uri

# ========= episode è£¡é¢ç”¨çš„ =========
def process_segments(series: str, ep: str, video_path: str, date: Any):
    s = safe_name(series)
    series_dir = VIDEO_ROOT / s
    series_dir.mkdir(parents=True, exist_ok=True)

    with VideoFileClip(video_path) as v:
        dur = v.duration

    start = 0
    idx = 0
    while start < dur - 5:
        end = min(start + SEG_LEN, dur)
        seg_mp4  = series_dir / f"segment_{s}_{ep}_seg{idx}.mp4"
        seg_json = series_dir / f"segment_{s}_{ep}_seg{idx}.json"
        hf_path  = f"videos/{s}/segment_{s}_{ep}_seg{idx}.mp4"

        if not seg_mp4.exists():
            with VideoFileClip(video_path) as v:
                v.subclipped(start, end).write_videofile(
                    str(seg_mp4),
                    codec="libx264",
                    audio_codec="aac",
                    logger=None,
                )

        if not seg_json.exists():
            file_uri = upload_file_to_gemini(str(seg_mp4))
            if not file_uri:
                log_error(f"segment upload {series} {ep} seg{idx}", "upload to gemini failed")
            else:
                # é€™è£¡ä¹Ÿç”¨ retryï¼Œæ¯ä¸€æ®µéƒ½æœƒå¹³å‡ä½¿ç”¨ä¸åŒ key
                def _call_segment(c):
                    return generate_segment_queries(client=c, file_uri=file_uri)

                q = retry(_call_segment, f"segment gen {series} {ep} seg{idx}")
                if q is not None:
                    seg_json.write_text(json.dumps({
                        "series_name": series,
                        "episode_id": ep,
                        "segment_index": idx,
                        "release_date": date,
                        "file_name": hf_path,
                        "query": q,
                    }, ensure_ascii=False, indent=2), encoding="utf-8")

        start += SEG_LEN - SEG_OVERLAP
        idx += 1

def process_episode(series: str, ep: str, video_path: str, date: Any):
    s = safe_name(series)
    series_dir = VIDEO_ROOT / s
    series_dir.mkdir(parents=True, exist_ok=True)

    ep_json = series_dir / f"episode_{s}_{ep}.json"
    ep_mp4  = series_dir / f"episode_{s}_{ep}.mp4"
    hf_path = f"videos/{s}/episode_{s}_{ep}.mp4"

    if not ep_mp4.exists():
        shutil.copy2(video_path, ep_mp4)

    if not ep_json.exists():
        file_uri = upload_file_to_gemini(str(ep_mp4))
        if not file_uri:
            log_error(f"episode upload {series} {ep}", "upload to gemini failed")
        else:
            def _call_episode(c):
                return generate_episode_queries(client=c, file_uri=file_uri)

            q = retry(_call_episode, f"episode {series} {ep}")
            if q is not None:
                ep_json.write_text(json.dumps({
                    "file_name": hf_path,
                    "series_name": series,
                    "episode_id": ep,
                    "release_date": date,
                    "query": q,
                }, ensure_ascii=False, indent=2), encoding="utf-8")

def run_one_episode(series: str, ep_info: Dict[str, Any]):
    ep_id = ep_info["episode_id"]
    video = ep_info["video_path"]
    date  = ep_info.get("release_date")
    process_segments(series, ep_id, video, date)
    process_episode(series, ep_id, video, date)

# ========= series ä¸Šå‚³ =========
def upload_one_series(series: str):
    api = HfApi(token=HF_TOKEN)
    s = safe_name(series)

    api.upload_large_folder(
        folder_path=str(CACHE_ROOT),
        repo_id=HF_SEG,
        repo_type="dataset",
        allow_patterns=[f"videos/{s}/segment_{s}_*.mp4"],
        commit_message=f"{series} segments batch",
    )

    api.upload_large_folder(
        folder_path=str(CACHE_ROOT),
        repo_id=HF_EP,
        repo_type="dataset",
        allow_patterns=[f"videos/{s}/episode_{s}_*.mp4"],
        commit_message=f"{series} episodes batch",
    )

    update_segment_metadata(HF_TOKEN)
    update_episode_metadata(HF_TOKEN)
    logging.info(f"âœ… uploaded whole series {series}")

# ========= series-level =========
def process_series(series: str, eps: List[Dict[str, Any]]):
    s = safe_name(series)
    series_dir = VIDEO_ROOT / s
    series_dir.mkdir(parents=True, exist_ok=True)

    series_json = series_dir / f"series_{s}.json"
    if series_json.exists():
        return

    series_mp4 = series_dir / f"series_{s}.mp4"
    if not series_mp4.exists():
        txt = series_dir / f"series_{s}.txt"
        with txt.open("w") as f:
            for e in eps:
                f.write(f"file '{Path(e['video_path']).absolute()}'\n")
        subprocess.run(
            ["ffmpeg","-f","concat","-safe","0","-i",str(txt),"-c","copy",str(series_mp4)],
            check=True
        )
        txt.unlink()

    low = series_dir / f"series_{s}_low_fps.mp4"
    if not low.exists():
        subprocess.run([
            "ffmpeg","-y","-i",str(series_mp4),
            "-vf","fps=0.2","-an","-c:v","libx264","-crf","32","-preset","veryfast",
            str(low)
        ], check=True)

    file_uri = upload_file_to_gemini(str(low))
    if file_uri:
        def _call_series(c):
            return generate_series_queries(client=c, file_uri=file_uri)
        time.sleep(1)
        series_query = retry(_call_series, f"series {series}") or {"error": "gen failed"}
    else:
        log_error(f"series upload {series}", "upload to gemini failed")
        series_query = {"error": "upload failed"}

    api = HfApi(token=HF_TOKEN)
    api.upload_file(
        path_or_fileobj=str(series_mp4),
        repo_id=HF_SER,
        path_in_repo=f"videos/{s}/series_{s}.mp4",
        repo_type="dataset",
    )

    series_json.write_text(json.dumps({
        "file_name": f"videos/{s}/series_{s}.mp4",
        "series_name": series,
        "query": series_query,
    }, ensure_ascii=False, indent=2), encoding="utf-8")

    update_series_metadata(HF_TOKEN)

# ========= dataset =========
def load_and_group_dataset() -> Dict[str, List[Dict[str, Any]]]:
    ds = load_dataset(DATASET, SUBSET, split="train").cast_column("video", Video(decode=False))
    groups: Dict[str, List[Dict[str, Any]]] = {}
    for r in ds:
        groups.setdefault(r["series_name"], []).append({
            "episode_id": r["episode_name"],
            "series_name": r["series_name"],
            "video_path": r["video"]["path"],
            "release_date": r.get("release_date"),
        })
    return groups

# ========= main =========
def main():
    # ç¢ºä¿ HF repo å­˜åœ¨
    for r in [HF_SEG, HF_EP, HF_SER]:
        create_repo(r, token=HF_TOKEN, repo_type="dataset", exist_ok=True)

    groups = load_and_group_dataset()

    for series, eps in groups.items():
        logging.info(f"=== {series} ===")

        # è·‘é€™å€‹ series çš„æ‰€æœ‰ episode
        for ep in eps:
            run_one_episode(series, ep)

        # ä¸Šå‚³é€™å€‹ series çš„ segment/episode
        upload_one_series(series)

        # å†åš series-level
        try:
            eps_sorted = sorted(eps, key=lambda e: float(e["episode_id"]))
        except Exception:
            eps_sorted = eps
        process_series(series, eps_sorted)

    logging.info("âœ… all done")

if __name__ == "__main__":
    main()
