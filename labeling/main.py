# simplified_main_autoupdate.py (multithread ç‰ˆ)
import os, json, time, logging, shutil, tempfile, subprocess, threading
from pathlib import Path
from typing import Any, Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed

from tqdm import tqdm
from dotenv import load_dotenv
from datasets import load_dataset, Dataset, Video
from huggingface_hub import HfApi, create_repo
import google.genai as genai
from moviepy import VideoFileClip

from segment_processor import generate_segment_queries, BlockedContentError
from episode_processor import generate_episode_queries
from series_processor import generate_series_queries
from update_metadata import (
    update_segment_metadata,
    update_episode_metadata,
    update_series_metadata,
)

# ================== åŸºæœ¬è¨­å®š ==================
logging.basicConfig(level=logging.INFO)
load_dotenv()

GEMINI_API_KEYS = [k.strip() for k in os.getenv("GEMINI_API_KEY", "").split(",") if k.strip()]
HF_TOKEN = os.getenv("HF_TOKEN", "")
if not GEMINI_API_KEYS:
    raise RuntimeError("è«‹å…ˆè¨­å®š GEMINI_API_KEY")
if not HF_TOKEN:
    raise RuntimeError("è«‹å…ˆè¨­å®š HF_TOKEN")

_api_key_lock = threading.Lock()
_current_key_index = 0

def get_next_api_key():
    global _current_key_index
    with _api_key_lock:
        key = GEMINI_API_KEYS[_current_key_index]
        _current_key_index = (_current_key_index + 1) % len(GEMINI_API_KEYS)
    return key

def get_client():
    return genai.Client(api_key=get_next_api_key())

# Hugging Face repositories
HF_REPO_SEGMENT = "TakalaWang/anime-2024-winter-segment-queries"
HF_REPO_EPISODE = "TakalaWang/anime-2024-winter-episode-queries"
HF_REPO_SERIES  = "TakalaWang/anime-2024-winter-series-queries"

CACHE_DIR = Path("./cache_gemini_video"); CACHE_DIR.mkdir(parents=True, exist_ok=True)
ERROR_LOG = Path("./error_log.jsonl")

TEST_DATASET = "JacobLinCool/anime-2024"
TEST_SPLIT = "winter"
SEGMENT_LENGTH, SEGMENT_OVERLAP = 60, 5

# retry ç›¸é—œ
MAX_RETRIES = 10
BASE_RETRY_SLEEP = 10  # sec
QUOTA_BACKOFF_MULTIPLIER = 10

# åŸ·è¡Œç·’æ•¸é‡ï¼Œå¯ä¾ç…§ä½ æ©Ÿå™¨èª¿
MAX_WORKERS = 4

# ================== å…±ç”¨å·¥å…· ==================
def call_with_retry(fn, *a, **kw):
    """
    quota/429: æ¯æ¬¡é‡è©¦ç¡æ›´ä¹…
    å…¶ä»–éŒ¯èª¤: ç”¨å›ºå®šçš„ BASE_RETRY_SLEEP
    """
    context = kw.pop("context", "æœªçŸ¥ä»»å‹™")
    print(f"Processing {context}")
    last_exc = None

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            return fn(*a, **kw)
        except Exception as e:
            last_exc = e
            msg = str(e)
            # ç²—ç•¥åµæ¸¬ quota / 429 / RESOURCE_EXHAUSTED
            is_quota = ("429" in msg) or ("quota" in msg.lower()) or ("RESOURCE_EXHAUSTED" in msg)
            if is_quota:
                sleep_sec = BASE_RETRY_SLEEP * (QUOTA_BACKOFF_MULTIPLIER ** (attempt - 1))
                print(f"âš ï¸ quota é¡žéŒ¯èª¤ï¼Œç¬¬ {attempt} æ¬¡ï¼Œ{sleep_sec}s å¾Œå†è©¦ï¼š{e}")
            else:
                sleep_sec = BASE_RETRY_SLEEP
                print(f"âŒ éž quota éŒ¯èª¤ï¼Œç¬¬ {attempt} æ¬¡ï¼š{e}")

            time.sleep(sleep_sec)

    # å…¨éƒ¨å¤±æ•—æ‰å¯« log
    ERROR_LOG.parent.mkdir(parents=True, exist_ok=True)
    with open(ERROR_LOG, "a", encoding="utf-8") as f:
        json.dump(
            {
                "time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "context": context,
                "error": str(last_exc),
            },
            f,
            ensure_ascii=False,
        )
        f.write("\n")

    print(f"ðŸš¨ é€™ç­†ä¸€ç›´å¤±æ•—ï¼Œè«‹çœ‹ error_log.jsonlï¼š{context}")
    return None

def down_video_fps(src: Path, dst: Path):
    subprocess.run([
        "ffmpeg","-y","-i",str(src),
        "-vf","fps=0.2","-an","-c:v","libx264","-crf","32","-preset","veryfast",str(dst)
    ], check=True)

def upload_video_to_gemini(client: genai.Client, video_path: str) -> str:
    path = Path(video_path)
    try:
        path.name.encode("ascii")
        upload_path = str(path)
    except UnicodeEncodeError:
        tmp = Path(tempfile.gettempdir()) / f"temp_{int(time.time()*1000)}{path.suffix}"
        shutil.copy2(video_path, tmp)
        upload_path = str(tmp)
    uploaded = call_with_retry(lambda: client.files.upload(file=upload_path), context=f"upload {video_path}")
    if uploaded is None:
        raise RuntimeError("æª”æ¡ˆä¸Šå‚³åˆ° Gemini å¤±æ•—")
    while uploaded.state.name == "PROCESSING":
        time.sleep(5)
        uploaded = client.files.get(name=uploaded.name)
    if uploaded.state.name == "FAILED":
        raise RuntimeError("å½±ç‰‡è™•ç†å¤±æ•—")
    return uploaded.uri, uploaded.name

def upload_video_to_hf(repo: str, file: Path, repo_path: str):
    api = HfApi(token=HF_TOKEN)
    api.upload_file(
        path_or_fileobj=str(file),
        repo_id=repo,
        path_in_repo=repo_path,
        repo_type="dataset",
    )

# ================== è™•ç†æµç¨‹ ==================
def load_and_group_dataset() -> Dict[str, List[Dict[str, Any]]]:
    ds = load_dataset(TEST_DATASET, TEST_SPLIT, split="train").cast_column("video", Video(decode=False))
    groups: Dict[str, List[Dict[str, Any]]] = {}
    for r in ds:
        groups.setdefault(r["series_name"], []).append({
            "episode_id": r["episode_name"],
            "series_name": r["series_name"],
            "video_path": r["video"]["path"],
            "release_date": r.get("release_date")
        })
    return groups

def process_segments(series: str, episode: str, path: str, dur: float, date: Any):
    updated = False
    results = []
    start = 0
    while start < dur - 5:
        end = min(start + SEGMENT_LENGTH, dur)
        safe = series.replace(" ", "_").replace("/", "_")
        seg_idx = len(results)
        seg_path = CACHE_DIR / f"segment_{safe}_{episode}_seg{seg_idx}.mp4"
        cache_path = seg_path.with_suffix(".json")

        if not seg_path.exists():
            with VideoFileClip(path) as v:
                v.subclipped(start, end).write_videofile(str(seg_path), codec="libx264", audio_codec="aac", logger=None)

        if cache_path.exists():
            with open(cache_path, "r", encoding="utf-8") as f:
                record = json.load(f)
            results.append(record)
            start += SEGMENT_LENGTH - SEGMENT_OVERLAP
            continue

        def gen():
            c = get_client()
            file_uri, file_name = upload_video_to_gemini(c, str(seg_path))
            try:
                query = generate_segment_queries(client=c, file_uri=file_uri)
            finally:
                # åˆªæŽ‰ Gemini çš„æª”æ¡ˆ
                try:
                    c.files.delete(name=file_name)
                except Exception:
                    pass
            return query

        try:
            data = call_with_retry(gen, context=f"segment {series} {episode} seg{seg_idx}")
        except BlockedContentError:
            data = {k: ["å…§å®¹è¢«é˜»æ­¢"]*3 for k in ["visual_saliency","character_emotion","action_behavior","dialogue","symbolic_scene"]}

        record = {
            "series_name": series,
            "episode_id": episode,
            "segment_index": seg_idx,
            "release_date": date,
            "file_name": f"videos/{safe}/segment_{safe}_{episode}_seg{seg_idx}.mp4",
            "query": data,
        }
        results.append(record)
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(record, f, ensure_ascii=False, indent=2)
        upload_video_to_hf(HF_REPO_SEGMENT, seg_path, record["file_name"])
        updated = True
        start += SEGMENT_LENGTH - SEGMENT_OVERLAP

    if updated:
        # å¤šåŸ·è¡Œç·’ä¸‹é€™å€‹å¯èƒ½æœƒè¢«å«å¾ˆå¤šæ¬¡ï¼Œä¸éŽæ˜¯ä½ åŽŸæœ¬çš„è¡Œç‚ºï¼Œå°±ä¿ç•™
        update_segment_metadata(HF_TOKEN)
    return results

def process_episode(series: str, episode: str, path: str, date: Any):
    safe = series.replace(" ", "_").replace("/", "_")
    cache_path = CACHE_DIR / f"episode_{safe}_{episode}.json"
    if cache_path.exists():
        with open(cache_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def gen():
        c = get_client()
        file_uri, file_name = upload_video_to_gemini(c, str(path))
        try:
            return generate_episode_queries(client=c, file_uri=file_uri)
        finally:
            try:
                c.files.delete(name=file_name)
            except Exception:
                pass

    query = call_with_retry(gen, context=f"episode {series} {episode}")

    hf_path = f"videos/{safe}/episode_{safe}_{episode}.mp4"
    upload_video_to_hf(HF_REPO_EPISODE, Path(path), hf_path)

    record = {
        "file_name": hf_path,
        "series_name": series,
        "episode_id": episode,
        "release_date": date,
        "query": query,
    }
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(record, f, ensure_ascii=False, indent=2)

    update_episode_metadata(HF_TOKEN)
    return record

def process_series(series: str, episodes: List[Dict[str, Any]]):
    safe = series.replace(" ", "_").replace("/", "_")
    cache_path = CACHE_DIR / f"series_{safe}.json"
    if cache_path.exists():
        with open(cache_path, "r", encoding="utf-8") as f:
            return json.load(f)

    series_video = CACHE_DIR / f"series_{safe}.mp4"
    if not series_video.exists():
        with open(series_video.with_suffix(".txt"), "w") as f:
            for e in episodes:
                f.write(f"file '{Path(e['video_path']).absolute()}'\n")
        subprocess.run([
            "ffmpeg","-f","concat","-safe","0","-i",str(series_video.with_suffix('.txt')),
            "-c","copy",str(series_video)
        ], check=True)
        series_video.with_suffix(".txt").unlink()

    low = CACHE_DIR / f"series_{safe}_low_fps.mp4"
    if not low.exists():
        down_video_fps(series_video, low)

    def gen():
        c = get_client()
        file_uri, file_name = upload_video_to_gemini(c, str(low))
        try:
            return generate_series_queries(client=c, file_uri=file_uri)
        finally:
            try:
                c.files.delete(name=file_name)
            except Exception:
                pass

    query = call_with_retry(gen, context=f"series {series}")
    upload_video_to_hf(HF_REPO_SERIES, series_video, f"videos/series_{safe}.mp4")

    date = sorted({e["release_date"] for e in episodes if e.get("release_date")})
    record = {
        "file_name": f"videos/series_{safe}.mp4",
        "series_name": series,
        "release_date": date[0] if date else None,
        "query": query,
    }
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(record, f, ensure_ascii=False, indent=2)

    update_series_metadata(HF_TOKEN)
    return record

# ================== ä¸»ç¨‹å¼ ==================
def main():
    print(f"ðŸ”‘ ä½¿ç”¨ {len(GEMINI_API_KEYS)} å€‹ Gemini API Keys")
    for r in [HF_REPO_SEGMENT, HF_REPO_EPISODE, HF_REPO_SERIES]:
        create_repo(r, token=HF_TOKEN, repo_type="dataset", exist_ok=True)

    groups = load_and_group_dataset()
    groups = dict(list(groups.items())[:10])

    for sname, eps in groups.items():
        print(f"ðŸŽ¬ ç³»åˆ—: {sname}")

        futures = []
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            for ep in eps:
                def job(ep=ep):
                    with VideoFileClip(ep["video_path"]) as v:
                        dur = v.duration
                    _ = process_segments(sname, ep["episode_id"], ep["video_path"], dur, ep.get("release_date"))
                    _ = process_episode(sname, ep["episode_id"], ep["video_path"], ep.get("release_date"))
                futures.append(executor.submit(job))

            for f in as_completed(futures):
                exc = f.exception()
                if exc:
                    print(f"âš ï¸ episode ä»»å‹™å‡ºéŒ¯ï¼š{exc}")
        
        eps = sorted(eps, key=lambda e: int(e["episode_id"]))
        _ = process_series(sname, eps)

    print("âœ… æ‰€æœ‰ç³»åˆ—è™•ç†å®Œæˆï¼Œmetadata çš†å·²è‡ªå‹•æ›´æ–°ã€‚")


if __name__ == "__main__":
    main()
