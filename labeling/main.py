import os
import json
import time
import logging
from pathlib import Path
from typing import Any, Dict, List, Tuple

from tqdm import tqdm
from dotenv import load_dotenv
from datasets import load_dataset, Dataset, Video
from huggingface_hub import HfApi, create_repo

import google.genai as genai
from moviepy import VideoFileClip, concatenate_videoclips


from segment_processor import generate_segment_queries
from episode_processor import generate_episode_queries
from series_processor import generate_series_queries


# ================== åŸºæœ¬è¨­å®š ==================
logging.basicConfig(level=logging.INFO)
load_dotenv()

# æ”¯æ´å–®å€‹æˆ–å¤šå€‹ API Keyï¼ˆç”¨é€—è™Ÿåˆ†éš”æˆ– JSON é™£åˆ—ï¼‰
GEMINI_API_KEYS = [
    k.strip() for k in os.getenv("GEMINI_API_KEY", "").strip().split(",") if k.strip()
]

HF_TOKEN = os.getenv("HF_TOKEN", "")

if not GEMINI_API_KEYS:
    raise RuntimeError("è«‹å…ˆè¨­å®š GEMINI_API_KEY")
if not HF_TOKEN:
    raise RuntimeError("è«‹å…ˆè¨­å®š HF_TOKEN")

# API Key è¼ªæ›
current_key_index = 0


def get_next_api_key():
    """è¼ªæ›ä½¿ç”¨ API Key ä»¥é¿å…é€Ÿç‡é™åˆ¶"""
    global current_key_index
    key = GEMINI_API_KEYS[current_key_index]
    current_key_index = (current_key_index + 1) % len(GEMINI_API_KEYS)
    return key


# Hugging Face å€‰åº«è¨­å®š
HF_REPO_SEGMENT = "TakalaWang/anime-2024-winter-segment-queries"
HF_REPO_EPISODE = "TakalaWang/anime-2024-winter-episode-queries"
HF_REPO_SERIES = "TakalaWang/anime-2024-winter-series-queries"

# å¿«å–ç›®éŒ„
CACHE_DIR = Path("./cache_gemini_video")
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# è³‡æ–™é›†è¨­å®š
TEST_DATASET = "JacobLinCool/anime-2024"
TEST_SPLIT = "winter"
SEGMENT_LENGTH = 60  # ç‰‡æ®µé•·åº¦ï¼ˆç§’ï¼‰
SEGMENT_OVERLAP = 5  # ç‰‡æ®µé‡ç–Šï¼ˆç§’ï¼‰
MAX_RETRIES = 5      # æœ€å¤§é‡è©¦æ¬¡æ•¸
RETRY_SLEEP = 5    # é‡è©¦ç­‰å¾…ç§’æ•¸


def get_client():
    """ç²å– Gemini clientï¼ˆä½¿ç”¨è¼ªæ›çš„ API keyï¼‰"""
    return genai.Client(api_key=get_next_api_key())


# ================== Hugging Face å·¥å…·å‡½æ•¸ ==================
def ensure_hf_repos():
    """ç¢ºä¿ Hugging Face å€‰åº«å­˜åœ¨"""
    create_repo(HF_REPO_SEGMENT, token=HF_TOKEN, repo_type="dataset", exist_ok=True)
    create_repo(HF_REPO_EPISODE, token=HF_TOKEN, repo_type="dataset", exist_ok=True)
    create_repo(HF_REPO_SERIES, token=HF_TOKEN, repo_type="dataset", exist_ok=True)


def upload_json_to_hf(repo_id: str, path: Path, repo_path: str):
    """ä¸Šå‚³ JSON æª”æ¡ˆåˆ° Hugging Face"""
    api = HfApi(token=HF_TOKEN)
    api.upload_file(
        path_or_fileobj=str(path),
        repo_id=repo_id,
        path_in_repo=repo_path,
        repo_type="dataset",
    )


def upload_dataset_to_hf(repo_id: str, data: List[Dict[str, Any]]):
    """ä¸Šå‚³æ•¸æ“šåˆ—è¡¨åˆ° HF dataset æ ¼å¼ï¼Œå•Ÿç”¨ data viewer"""

    # å‰µå»º Dataset å°è±¡ï¼ˆç›´æ¥å¾å­—å…¸åˆ—è¡¨ï¼Œä¿ç•™åµŒå¥—çµæ§‹ï¼‰
    dataset = Dataset.from_list(data)

    # ä¸Šå‚³åˆ° HF
    dataset.push_to_hub(repo_id, token=HF_TOKEN)

    print(f"ğŸ“Š å·²ä¸Šå‚³ dataset åˆ° HF: {repo_id} ({len(data)} ç­†è¨˜éŒ„)")


# ================== æ•¸æ“šé›†ç®¡ç† ==================
def create_metadata_jsonl(
    repo_id: str,
    metadata_list: List[Dict[str, Any]],
    metadata_filename: str = "metadata.jsonl",
):
    """å‰µå»º metadata.jsonl æ–‡ä»¶ä¸¦ä¸Šå‚³åˆ° HF datasetï¼Œå•Ÿç”¨ data viewer"""
    # ç¢ºä¿æ‰€æœ‰è¨˜éŒ„éƒ½æœ‰ file_name å­—æ®µ
    processed_metadata = []
    for item in metadata_list:
        if "file_name" not in item:
            # å¦‚æœæ²’æœ‰ file_nameï¼Œå˜—è©¦å¾å…¶ä»–å­—æ®µæ¨æ–·
            if "episode_name" in item:
                item["file_name"] = (
                    f"videos/{item.get('series_name', 'unknown')}/episode_{item['episode_name']}.mp4"
                )
            elif "segment_index" in item:
                item["file_name"] = (
                    f"videos/segment_{item.get('episode_id', 'unknown')}_seg{item['segment_index']}.mp4"
                )
            else:
                continue  # è·³éæ²’æœ‰æ–‡ä»¶åçš„è¨˜éŒ„

        processed_metadata.append(item)

    # å¯«å…¥æœ¬åœ° metadata.jsonl æ–‡ä»¶
    metadata_path = CACHE_DIR / metadata_filename
    with open(metadata_path, "w", encoding="utf-8") as f:
        for item in processed_metadata:
            json.dump(item, f, ensure_ascii=False)
            f.write("\n")

    # ä¸Šå‚³åˆ° HF
    api = HfApi(token=HF_TOKEN)
    api.upload_file(
        path_or_fileobj=str(metadata_path),
        repo_id=repo_id,
        path_in_repo=metadata_filename,
        repo_type="dataset",
    )

    print(
        f"ğŸ“‹ å·²å‰µå»ºä¸¦ä¸Šå‚³ metadata.jsonl åˆ° {repo_id} ({len(processed_metadata)} ç­†è¨˜éŒ„)"
    )
    return processed_metadata


def upload_video_to_hf(repo_id: str, video_path: Path, repo_path: str):
    """ä¸Šå‚³å½±ç‰‡æª”æ¡ˆåˆ° HuggingFace"""
    api = HfApi(token=HF_TOKEN)
    api.upload_file(
        path_or_fileobj=str(video_path),
        repo_id=repo_id,
        path_in_repo=repo_path,
        repo_type="dataset",
    )
    print(f"ğŸ“¤ å·²ä¸Šå‚³å½±ç‰‡åˆ° HF: {repo_path}")


# ================== å½±ç‰‡è™•ç†å·¥å…·å‡½æ•¸ ==================
def extract_video_segment(
    video_path: str, start_s: float, end_s: float, output_path: Path
):
    with VideoFileClip(video_path) as video:
        segment = video.subclipped(start_s, end_s)
        segment.write_videofile(
            str(output_path),
            codec="libx264",
            audio_codec="aac",
            temp_audiofile=str(
                output_path.parent / f"temp_{output_path.stem}_audio.m4a"
            ),
            remove_temp=True,
            logger=None,
        )


def concatenate_videos(video_paths: List[str], output_path: Path):
    """åˆä½µå¤šå€‹å½±ç‰‡"""
    clips = []
    for path in video_paths:
        clips.append(VideoFileClip(path))

    final_clip = concatenate_videoclips(clips, method="compose")
    final_clip.write_videofile(
        str(output_path),
        codec="libx264",
        audio_codec="aac",
        temp_audiofile=str(output_path.parent / f"temp_{output_path.stem}_audio.m4a"),
        remove_temp=True,
        logger=None,
    )

    # é—œé–‰æ‰€æœ‰ clips
    for clip in clips:
        clip.close()
    final_clip.close()

    print(f"ğŸ”— å·²åˆä½µå½±ç‰‡: {output_path.name}")


def get_video_duration_from_path(path: str) -> float:
    """ç²å–å½±ç‰‡é•·åº¦ï¼ˆç§’ï¼‰"""
    with VideoFileClip(path) as clip:
        return clip.duration


# ================== API å·¥å…·å‡½æ•¸ ==================
def upload_video_to_gemini(client: genai.Client, video_path: str) -> str:
    """
    ä¸Šå‚³å½±ç‰‡åˆ° Gemini API ä¸¦ç­‰å¾…è™•ç†å®Œæˆ
    
    Args:
        client: Gemini API å®¢æˆ¶ç«¯
        video_path: å½±ç‰‡æª”æ¡ˆè·¯å¾‘
        
    Returns:
        file_uri: Gemini è™•ç†å®Œæˆçš„æª”æ¡ˆ URI
    """
    uploaded = client.files.upload(file=video_path)
    file_uri = uploaded.uri

    while uploaded.state.name == "PROCESSING":
        time.sleep(1)
        uploaded = client.files.get(name=uploaded.name)

    if uploaded.state.name == "FAILED":
        raise ValueError(f"å½±ç‰‡è™•ç†å¤±æ•—: {uploaded.state.name}")
    
    return file_uri


def call_with_retry(fn, *args, **kwargs):
    """åŸ·è¡Œ API å‘¼å«ï¼Œå¤±æ•—æ™‚è‡ªå‹•æ›´æ› Gemini Key ä¸¦é‡è©¦"""
    for attempt in range(MAX_RETRIES):
        try:
            return fn(*args, **kwargs)

        except Exception as e:
            msg = str(e).lower()
            is_rate_limited = (
                "429" in msg or
                "quota" in msg or
                "rate" in msg or
                "exceeded" in msg
            )

            if is_rate_limited:
                print(f"[retry {attempt+1}/{MAX_RETRIES}] Rate limited -> æ›ä¸‹ä¸€å€‹ API Key")
                time.sleep(RETRY_SLEEP)

                # é‡æ–°å»ºç«‹ client
                new_client = get_client()
                kwargs["client"] = new_client
                continue

            print(f"[error] {type(e).__name__}: {e}")
            raise

    raise RuntimeError(f"é‡è©¦æ¬¡æ•¸å·²é”ä¸Šé™ ({MAX_RETRIES})ï¼Œä»æœªæˆåŠŸã€‚")


# ================== è³‡æ–™è™•ç†å‡½æ•¸ ==================
def load_and_group_dataset() -> Dict[str, List[Dict[str, Any]]]:
    print("è¼‰å…¥è³‡æ–™é›†...")
    ds = load_dataset(TEST_DATASET, TEST_SPLIT, split="train")
    ds = ds.cast_column("video", Video(decode=False))

    series_groups: Dict[str, List[Dict[str, Any]]] = {}
    for row in tqdm(ds, desc="group by series"):
        series_name = row["series_name"]
        episode_name = row["episode_name"]
        video_path = row["video"]["path"]
        release_date = row.get("release_date")

        series_groups.setdefault(series_name, []).append(
            {
                "episode_name": episode_name,
                "series_name": series_name,
                "video_path": video_path,
                "release_date": release_date,
            }
        )

    return series_groups


def process_segments_for_episode(
    series_name: str,
    episode_id: str,
    video_path: str,
    duration_s: float,
    release_date: Any,
) -> List[Dict[str, Any]]:
    """è™•ç†å–®é›†çš„ç‰‡æ®µç´šåˆ¥æŸ¥è©¢ç”Ÿæˆ"""
    segment_ranges = []
    start = 0.0
    while start < duration_s:
        end = min(start + SEGMENT_LENGTH, duration_s)
        if end - start < 5:
            break
        segment_ranges.append((start, end))
        if end >= duration_s:
            break
        start = start + SEGMENT_LENGTH - SEGMENT_OVERLAP

    segment_files = []
    for seg_idx, (s, e) in enumerate(
        tqdm(segment_ranges, desc=f"åˆ‡å‰²ç‰‡æ®µ {episode_id}", unit="seg")
    ):
        segment_video_path = CACHE_DIR / f"segment_{episode_id}_seg{seg_idx}.mp4"
        if not segment_video_path.exists():
            extract_video_segment(video_path, s, e, segment_video_path)

        segment_files.append(
            {
                "index": seg_idx,
                "path": segment_video_path,
            }
        )

    seg_results = []
    for seg_info in segment_files:
        seg_idx = seg_info["index"]
        segment_path = seg_info["path"]

        cache_path = CACHE_DIR / f"segment_{episode_id}_seg{seg_idx}.json"
        if cache_path.exists():
            # å°±ç®—æœ‰ cacheï¼Œä¹Ÿå¹«å®ƒè£œä¸Š series_name / release_dateï¼Œé¿å…èˆŠæª”æ˜¯ç©ºçš„
            with open(cache_path, "r", encoding="utf-8") as f:
                cached = json.load(f)
            cached["series_name"] = series_name
            cached["release_date"] = release_date
            seg_results.append(cached)
            # å›å¯«ä¸€æ¬¡ï¼Œè®“æª”æ¡ˆä¹Ÿè®Šæˆæ–°çš„
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(cached, f, ensure_ascii=False, indent=2)
            continue

        client = get_client()
        file_uri = call_with_retry(upload_video_to_gemini, client=client, video_path=str(segment_path))
        
        data = call_with_retry(
            generate_segment_queries,
            client=client,
            file_uri=file_uri,
        )

        record = {
            "series_name": series_name,
            "episode_id": episode_id,
            "segment_index": seg_idx,
            "release_date": release_date,
            "file_name": f"videos/segment_{episode_id}_seg{seg_idx}.mp4",
            "queries": data,
        }

        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(record, f, ensure_ascii=False, indent=2)

        seg_results.append(record)

        upload_video_to_hf(
            HF_REPO_SEGMENT,
            segment_path,
            record["file_name"]
        )

    return seg_results


def process_episode_level(
    series_name: str,
    episode_id: str,
    video_path: str,
    duration_s: float,
    release_date: Any,
) -> Dict[str, Any]:
    """è™•ç†é›†æ•¸ç´šåˆ¥çš„æŸ¥è©¢ç”Ÿæˆ"""
    client = get_client()
    file_uri = call_with_retry(upload_video_to_gemini, client=client, video_path=str(video_path))

    epi_result = call_with_retry(
        generate_episode_queries,
        client=client,
        file_uri=file_uri,
    )

    # ä¸Šå‚³å®Œæ•´é›†æ•¸å½±ç‰‡åˆ° HF
    episode_video_hf_path = f"videos/{series_name}/episode_{episode_id}.mp4"
    print("  ğŸ“¤ ä¸Šå‚³å®Œæ•´é›†æ•¸åˆ° HuggingFace...")
    upload_video_to_hf(HF_REPO_EPISODE, Path(video_path), episode_video_hf_path)

    # åŒ…è£ episode metadata èˆ‡æ¨¡å‹å›æ‡‰
    episode_record = {
        "file_name": episode_video_hf_path,  # æ·»åŠ  file_name å­—æ®µç”¨æ–¼ data viewer
        "series_name": series_name,
        "episode_name": episode_id,
        "release_date": release_date,
        "duration": duration_s,
        "model_response": epi_result,
    }

    # ä»ç„¶ä¿å­˜å–®å€‹ JSON æ–‡ä»¶ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
    epi_local = CACHE_DIR / f"episode_{episode_id}.json"
    with open(epi_local, "w", encoding="utf-8") as f:
        json.dump(episode_record, f, ensure_ascii=False, indent=2)
    upload_json_to_hf(HF_REPO_EPISODE, epi_local, f"episode_{episode_id}.json")

    return episode_record


def process_single_episode(
    series_name: str, episode_info: Dict[str, Any]
) -> Tuple[str, str, float, Any, Dict[str, Any], List[Dict[str, Any]]]:
    episode_id = episode_info["episode_name"]
    video_path = episode_info["video_path"]
    release_date = episode_info.get("release_date")

    duration_s = get_video_duration_from_path(video_path)

    print(f"\n{'='*60}")
    print(f"è™•ç†é›†æ•¸: {episode_id}")
    print(f"å½±ç‰‡é•·åº¦: {duration_s:.2f} ç§’")
    print(f"{'='*60}\n")

    # ===== Segment level =====
    seg_results = process_segments_for_episode(
        series_name,
        episode_id,
        video_path,
        duration_s,
        release_date,
    )

    seg_local = CACHE_DIR / f"segment_{episode_id}.json"
    with open(seg_local, "w", encoding="utf-8") as f:
        json.dump(seg_results, f, ensure_ascii=False, indent=2)
    upload_json_to_hf(HF_REPO_SEGMENT, seg_local, f"segment_{episode_id}.json")

    # ===== Episode level =====
    episode_record = process_episode_level(
        series_name,
        episode_id,
        video_path,
        duration_s,
        release_date,
    )

    return (
        episode_id,
        video_path,
        duration_s,
        release_date,
        episode_record,
        seg_results,
    )



def process_series_level(
    series_name: str, processed_episodes: List[Tuple[str, str, float, Any]]
) -> Dict[str, Any]:
    """è™•ç†ç³»åˆ—ç´šåˆ¥çš„æŸ¥è©¢ç”Ÿæˆ"""
    
    # åˆä½µä¸¦ä¸Šå‚³æ•´å­£å½±ç‰‡
    print("  æº–å‚™æ•´å­£å½±ç‰‡...")
    episode_video_paths = [vp for _, vp, _, _ in processed_episodes]
    series_video_path = CACHE_DIR / f"series_{series_name}.mp4"
    if not series_video_path.exists():
        print("  ğŸ”— é–‹å§‹åˆä½µæ•´å­£å½±ç‰‡...")
        concatenate_videos(episode_video_paths, series_video_path)

    # ä¸Šå‚³æ•´å­£å½±ç‰‡åˆ° Gemini API é€²è¡Œåˆ†æ
    print("  ğŸ¤– ä½¿ç”¨ Gemini åˆ†ææ•´å­£å…§å®¹...")
    client = get_client()
    file_uri = call_with_retry(upload_video_to_gemini, client=client, video_path=str(series_video_path))
    
    series_result = call_with_retry(
        generate_series_queries,
        client=client,
        file_uri=file_uri,
    )

    print("  ğŸ“¤ ä¸Šå‚³æ•´å­£å½±ç‰‡åˆ° HuggingFace...")
    upload_video_to_hf(
        HF_REPO_SERIES, series_video_path, f"videos/series_{series_name}.mp4"
    )

    # å»ºç«‹ series metadataï¼ˆä¸è¦å­˜ episode_nameï¼Œåƒ…å­˜ release_dates èˆ‡æ¨¡å‹å›æ‡‰ï¼‰
    release_dates = sorted(
        {rd for (_eid, _vp, _dur, rd) in processed_episodes if rd is not None}
    )
    series_record = {
        "file_name": f"videos/series_{series_name}.mp4",  # æ·»åŠ  file_name å­—æ®µç”¨æ–¼ data viewer
        "series_name": series_name,
        "episode_count": len(processed_episodes),
        "release_dates": release_dates,
        "model_response": series_result,
    }

    series_local = CACHE_DIR / f"series_{series_name}.json"
    with open(series_local, "w", encoding="utf-8") as f:
        json.dump(series_record, f, ensure_ascii=False, indent=2)
    upload_json_to_hf(HF_REPO_SERIES, series_local, f"series_{series_name}.json")

    return series_record


def process_single_series(series_name: str, episodes: List[Dict[str, Any]]) -> Tuple[
    Tuple[str, List[Tuple[str, str, float, Any]]],
    List[Dict[str, Any]],
    List[Dict[str, Any]],
    Dict[str, Any],
]:
    """è™•ç†å–®å€‹ç³»åˆ—çš„æ‰€æœ‰é›†æ•¸ï¼Œè¿”å› (result, episode_metadata, segment_metadata, series_metadata)"""
    print(f"é–‹å§‹è™•ç†ç³»åˆ—: {series_name} (å…± {len(episodes)} é›†)")

    processed_episodes = []
    episode_metadata = []
    segment_metadata = []

    for episode_info in tqdm(episodes, desc=f"episodes - {series_name}", unit="ep"):
        episode_id, video_path, duration_s, release_date, episode_record, seg_results = (
            process_single_episode(series_name, episode_info)
        )
        processed_episodes.append((episode_id, video_path, duration_s, release_date))
        episode_metadata.append(episode_record)
        segment_metadata.extend(seg_results)

    # è™•ç†ç³»åˆ—ç´šåˆ¥
    series_record = process_series_level(series_name, processed_episodes)

    return (
        (series_name, processed_episodes),
        episode_metadata,
        segment_metadata,
        series_record,
    )


def process_all_series(
    series_groups: Dict[str, List[Dict[str, Any]]],
) -> Tuple[
    List[Tuple[str, List[Tuple[str, str, float, Any]]]],
    List[Dict[str, Any]],
    List[Dict[str, Any]],
    List[Dict[str, Any]],
]:
    """è™•ç†æ‰€æœ‰ç³»åˆ—ï¼Œè¿”å› (processed_series, all_episode_metadata, all_segment_metadata, all_series_metadata)"""
    processed_series = []
    all_episode_metadata = []
    all_segment_metadata = []
    all_series_metadata = []

    for series_name, episodes in series_groups.items():
        result, episode_metadata, segment_metadata, series_metadata = (
            process_single_series(series_name, episodes)
        )
        processed_series.append(result)
        all_episode_metadata.extend(episode_metadata)
        all_segment_metadata.extend(segment_metadata)
        all_series_metadata.append(series_metadata)

    return (
        processed_series,
        all_episode_metadata,
        all_segment_metadata,
        all_series_metadata,
    )


# ================== ä¸»ç¨‹å¼ ==================
def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    ensure_hf_repos()

    print(f"å·²è¨­å®š {len(GEMINI_API_KEYS)} å€‹ Gemini API Key")

    # è¼‰å…¥ä¸¦åˆ†çµ„è³‡æ–™é›†
    series_groups = load_and_group_dataset()
    series_groups = {k: series_groups[k] for k in list(series_groups)[:5]}

    # è™•ç†æ‰€æœ‰ç³»åˆ—èˆ‡ episodes
    (
        processed_series,
        all_episode_metadata,
        all_segment_metadata,
        all_series_metadata,
    ) = process_all_series(series_groups)

    # å‰µå»º metadata.jsonl æ–‡ä»¶ä»¥å•Ÿç”¨ Dataset Viewer
    if all_episode_metadata:
        create_metadata_jsonl(HF_REPO_EPISODE, all_episode_metadata, "metadata.jsonl")

    if all_segment_metadata:
        create_metadata_jsonl(HF_REPO_SEGMENT, all_segment_metadata, "metadata.jsonl")

    if all_series_metadata:
        create_metadata_jsonl(HF_REPO_SERIES, all_series_metadata, "metadata.jsonl")

    # å®Œæˆç¸½çµ
    total_series = len(processed_series)
    total_episodes = sum(len(eps) for _, eps in processed_series)

    print("\n" + "=" * 60)
    print("ğŸ‰ è™•ç†å®Œæˆï¼")
    print(f"âœ… è™•ç†äº† {total_series} å€‹ç³»åˆ—ï¼Œ{total_episodes} é›†")
    print(f"ğŸ“Š Episode metadata: {len(all_episode_metadata)} ç­†")
    print(f"ğŸ“Š Segment metadata: {len(all_segment_metadata)} ç­†")
    print("ğŸ” Dataset Viewer ç¾å·²å•Ÿç”¨ï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
