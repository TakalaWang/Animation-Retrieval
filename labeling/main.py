import os
import json
import time
import logging
import shutil
import tempfile
import subprocess
from pathlib import Path
from typing import Any, Dict, List, Tuple

from tqdm import tqdm
from dotenv import load_dotenv
from datasets import load_dataset, Dataset, Video
from huggingface_hub import HfApi, create_repo

import google.genai as genai
from moviepy import VideoFileClip


from segment_processor import generate_segment_queries, BlockedContentError
from episode_processor import generate_episode_queries
from series_processor import generate_series_queries
from update_metadata import update_segment_metadata, update_episode_metadata, update_series_metadata


# ================== åŸºæœ¬è¨­å®š ==================
logging.basicConfig(level=logging.INFO)
load_dotenv()

# æ”¯æ´å–®å€‹æˆ–å¤šå€‹ API Keyï¼ˆç”¨é€—è™Ÿåˆ†éš”æˆ– JSON é™£åˆ—ï¼‰
GEMINI_API_KEYS = [
    k.strip() for k in os.getenv("GEMINI_API_KEY", "").strip().split(",") if k.strip()
]

HF_TOKEN = os.getenv("HF_TOKEN", "")

if not GEMINI_API_KEYS:
    raise RuntimeError("è«‹å…ˆè¨­å®š GEMINI_API_KEY")
if not HF_TOKEN:
    raise RuntimeError("è«‹å…ˆè¨­å®š HF_TOKEN")

# API Key è¼ªæ›
current_key_index = 0


def get_next_api_key():
    """è¼ªæ›ä½¿ç”¨ API Key ä»¥é¿å…é€Ÿç‡é™åˆ¶"""
    global current_key_index
    key = GEMINI_API_KEYS[current_key_index]
    current_key_index = (current_key_index + 1) % len(GEMINI_API_KEYS)
    return key


# Hugging Face å€‰åº«è¨­å®š
HF_REPO_SEGMENT = "TakalaWang/anime-2024-winter-segment-queries"
HF_REPO_EPISODE = "TakalaWang/anime-2024-winter-episode-queries"
HF_REPO_SERIES = "TakalaWang/anime-2024-winter-series-queries"

# å¿«å–ç›®éŒ„
CACHE_DIR = Path("./cache_gemini_video")
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# è³‡æ–™é›†è¨­å®š
TEST_DATASET = "JacobLinCool/anime-2024"
TEST_SPLIT = "winter"
SEGMENT_LENGTH = 60  # ç‰‡æ®µé•·åº¦ï¼ˆç§’ï¼‰
SEGMENT_OVERLAP = 5  # ç‰‡æ®µé‡ç–Šï¼ˆç§’ï¼‰
MAX_RETRIES = 5      # æœ€å¤§é‡è©¦æ¬¡æ•¸
RETRY_SLEEP = 5    # é‡è©¦ç­‰å¾…ç§’æ•¸


def get_client():
    """ç²å– Gemini clientï¼ˆä½¿ç”¨è¼ªæ›çš„ API keyï¼‰"""
    return genai.Client(api_key=get_next_api_key())


# ================== Hugging Face å·¥å…·å‡½æ•¸ ==================
def ensure_hf_repos():
    """ç¢ºä¿ Hugging Face å€‰åº«å­˜åœ¨"""
    create_repo(HF_REPO_SEGMENT, token=HF_TOKEN, repo_type="dataset", exist_ok=True)
    create_repo(HF_REPO_EPISODE, token=HF_TOKEN, repo_type="dataset", exist_ok=True)
    create_repo(HF_REPO_SERIES, token=HF_TOKEN, repo_type="dataset", exist_ok=True)


def upload_dataset_to_hf(repo_id: str, data: List[Dict[str, Any]]):
    """ä¸Šå‚³æ•¸æ“šåˆ—è¡¨åˆ° HF dataset æ ¼å¼ï¼Œå•Ÿç”¨ data viewer"""

    # å‰µå»º Dataset å°è±¡ï¼ˆç›´æ¥å¾å­—å…¸åˆ—è¡¨ï¼Œä¿ç•™åµŒå¥—çµæ§‹ï¼‰
    dataset = Dataset.from_list(data)

    # ä¸Šå‚³åˆ° HF
    dataset.push_to_hub(repo_id, token=HF_TOKEN)

    print(f"ğŸ“Š å·²ä¸Šå‚³ dataset åˆ° HF: {repo_id} ({len(data)} ç­†è¨˜éŒ„)")


# ================== æ•¸æ“šé›†ç®¡ç† ==================
def create_metadata_jsonl(
    repo_id: str,
    metadata_list: List[Dict[str, Any]],
    metadata_filename: str = "metadata.jsonl",
):
    """å‰µå»º metadata.jsonl æ–‡ä»¶ä¸¦ä¸Šå‚³åˆ° HF datasetï¼Œå•Ÿç”¨ data viewer"""
    # è™•ç†æ¯ç­†è¨˜éŒ„ï¼Œç¢ºä¿æ ¼å¼æ­£ç¢º
    processed_metadata = []
    for item in metadata_list:
        # å‰µå»ºæ–°çš„è¨˜éŒ„ï¼ŒåŒ…å«æ‰€æœ‰å­—æ®µ
        record = {}
        
        # ç¢ºä¿æœ‰ file_name
        if "file_name" not in item:
            if "episode_name" in item:
                item["file_name"] = (
                    f"videos/{item.get('series_name', 'unknown')}/episode_{item['episode_name']}.mp4"
                )
            elif "segment_index" in item and "episode_id" in item:
                item["file_name"] = (
                    f"videos/segment_{item['episode_id']}_seg{item['segment_index']}.mp4"
                )
            else:
                print(f"âš ï¸  è­¦å‘Š: è·³éæ²’æœ‰ file_name çš„è¨˜éŒ„: {item.keys()}")
                continue

        # è¤‡è£½æ‰€æœ‰å­—æ®µåˆ°æ–°è¨˜éŒ„
        for key, value in item.items():
            record[key] = value

        processed_metadata.append(record)

    # å¯«å…¥æœ¬åœ° metadata.jsonl æ–‡ä»¶
    metadata_path = CACHE_DIR / metadata_filename
    with open(metadata_path, "w", encoding="utf-8") as f:
        for item in processed_metadata:
            json.dump(item, f, ensure_ascii=False)
            f.write("\n")

    # ä¸Šå‚³åˆ° HF
    api = HfApi(token=HF_TOKEN)
    api.upload_file(
        path_or_fileobj=str(metadata_path),
        repo_id=repo_id,
        path_in_repo=metadata_filename,
        repo_type="dataset",
    )

    print(
        f"ğŸ“‹ å·²å‰µå»ºä¸¦ä¸Šå‚³ metadata.jsonl åˆ° {repo_id} ({len(processed_metadata)} ç­†è¨˜éŒ„)"
    )
    return processed_metadata


def upload_video_to_hf(repo_id: str, video_path: Path, repo_path: str):
    """ä¸Šå‚³å½±ç‰‡æª”æ¡ˆåˆ° HuggingFace"""
    api = HfApi(token=HF_TOKEN)
    api.upload_file(
        path_or_fileobj=str(video_path),
        repo_id=repo_id,
        path_in_repo=repo_path,
        repo_type="dataset",
    )
    print(f"ğŸ“¤ å·²ä¸Šå‚³å½±ç‰‡åˆ° HF: {repo_path}")


# ================== å½±ç‰‡è™•ç†å·¥å…·å‡½æ•¸ ==================
def extract_video_segment(
    video_path: str, start_s: float, end_s: float, output_path: Path
):
    with VideoFileClip(video_path) as video:
        segment = video.subclipped(start_s, end_s)
        segment.write_videofile(
            str(output_path),
            codec="libx264",
            audio_codec="aac",
            temp_audiofile=str(
                output_path.parent / f"temp_{output_path.stem}_audio.m4a"
            ),
            remove_temp=True,
            logger=None,
        )


def down_video_fps(src_path: Path, dst_path: Path):
    """
    æŠŠå·²ç¶“ concat å¥½çš„åŸå§‹å¤§æª”å£“ä¸€å€‹å°ç‰ˆï¼Œå°ˆé–€çµ¦ Gemini ç”¨
    - é™ FPS
    - å¯é¸æ“‡é™è§£æåº¦
    """
    dst_path = Path(dst_path)
    cmd = [
        "ffmpeg",
        "-y",                       # è‡ªå‹•è¦†å¯«
        "-i", str(src_path),        # è¼¸å…¥å½±ç‰‡
        "-vf", "fps=0.2",
        "-an",                      # ç§»é™¤éŸ³è¨Š
        "-c:v", "libx264",          # ä½¿ç”¨ x264 ç·¨ç¢¼
        "-preset", "veryfast",      # åŠ å¿«ç·¨ç¢¼é€Ÿåº¦ï¼ˆç¨é™å£“ç¸®æ•ˆç‡ï¼‰
        str(dst_path),
    ]
    subprocess.run(cmd, check=True)


def get_video_duration_from_path(path: str) -> float:
    """ç²å–å½±ç‰‡é•·åº¦ï¼ˆç§’ï¼‰"""
    with VideoFileClip(path) as clip:
        return clip.duration


# ================== API å·¥å…·å‡½æ•¸ ==================
def upload_video_to_gemini(client: genai.Client, video_path: str) -> str:
    """
    ä¸Šå‚³å½±ç‰‡åˆ° Gemini API ä¸¦ç­‰å¾…è™•ç†å®Œæˆ
    
    Args:
        client: Gemini API å®¢æˆ¶ç«¯
        video_path: å½±ç‰‡æª”æ¡ˆè·¯å¾‘
        
    Returns:
        file_uri: Gemini è™•ç†å®Œæˆçš„æª”æ¡ˆ URI
    """
    original_path = Path(video_path)
    print(f"    ğŸ“¤ ä¸Šå‚³å½±ç‰‡: {original_path.name}")
    
    # æª¢æŸ¥æª”åæ˜¯å¦åŒ…å«é ASCII å­—ç¬¦ï¼ˆå¦‚ä¸­æ–‡ï¼‰
    filename = original_path.name
    temp_file = None
    upload_path = video_path
    
    try:
        # å˜—è©¦ç”¨ ASCII ç·¨ç¢¼æª”åï¼Œå¦‚æœå¤±æ•—å‰‡è¡¨ç¤ºæœ‰é ASCII å­—ç¬¦
        filename.encode('ascii')
    except UnicodeEncodeError:
        # æª”ååŒ…å«ä¸­æ–‡æˆ–å…¶ä»–é ASCII å­—ç¬¦ï¼Œéœ€è¦å‰µå»ºè‡¨æ™‚å‰¯æœ¬
        
        # ä½¿ç”¨æª”æ¡ˆçš„å¾Œç¶´åå’Œä¸€å€‹å®‰å…¨çš„ ASCII åç¨±
        safe_name = f"temp_upload_{int(time.time() * 1000)}{original_path.suffix}"
        temp_file = Path(tempfile.gettempdir()) / safe_name
        
        # è¤‡è£½æª”æ¡ˆåˆ°è‡¨æ™‚ä½ç½®
        shutil.copy2(video_path, temp_file)
        upload_path = str(temp_file)
    
    try:
        # æ¯æ¬¡éƒ½é‡æ–°ä¸Šå‚³ï¼Œé¿å…æ–‡ä»¶éæœŸå•é¡Œ
        def do_upload():
            return client.files.upload(file=upload_path)
        uploaded = call_with_retry(do_upload)
        file_uri = uploaded.uri
        
        while uploaded.state.name == "PROCESSING":
            time.sleep(5)
            uploaded = client.files.get(name=uploaded.name)

        if uploaded.state.name == "FAILED":
            raise ValueError(f"å½±ç‰‡è™•ç†å¤±æ•—: {uploaded.state.name}")
        
        print(f"    âœ… å®Œæˆ")
        return file_uri
    
    finally:
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        if temp_file and temp_file.exists():
            try:
                temp_file.unlink()
                print(f"    ğŸ—‘ï¸  å·²åˆªé™¤è‡¨æ™‚æª”æ¡ˆ")
            except Exception as e:
                print(f"    âš ï¸  ç„¡æ³•åˆªé™¤è‡¨æ™‚æª”æ¡ˆ: {e}")


def call_with_retry(fn, *args, **kwargs):
    """åŸ·è¡Œ API å‘¼å«ï¼Œå¤±æ•—æ™‚è‡ªå‹•æ›´æ› Gemini Key ä¸¦é‡è©¦"""
    for _ in range(MAX_RETRIES):
        try:
            # å¦‚æœ fn æ˜¯ä¸€å€‹ç„¡åƒæ•¸çš„é–‰åŒ…å‡½æ•¸ï¼Œç›´æ¥èª¿ç”¨
            if callable(fn) and not args and not kwargs:
                return fn()
            else:
                return fn(*args, **kwargs)
            
        except Exception as e:
            print(f"âŒ [error] {type(e).__name__}: {e}")
            time.sleep(RETRY_SLEEP)
            continue

    raise RuntimeError(f"é‡è©¦æ¬¡æ•¸å·²é”ä¸Šé™ ({MAX_RETRIES})ï¼Œä»æœªæˆåŠŸã€‚")


# ================== è³‡æ–™è™•ç†å‡½æ•¸ ==================
def load_and_group_dataset() -> Dict[str, List[Dict[str, Any]]]:
    print("è¼‰å…¥è³‡æ–™é›†...")
    ds = load_dataset(TEST_DATASET, TEST_SPLIT, split="train")
    ds = ds.cast_column("video", Video(decode=False))

    series_groups: Dict[str, List[Dict[str, Any]]] = {}
    for row in tqdm(ds, desc="group by series"):
        series_name = row["series_name"]
        episode_id = row["episode_name"]
        video_path = row["video"]["path"]
        release_date = row.get("release_date")

        series_groups.setdefault(series_name, []).append(
            {
                "episode_id": episode_id,
                "series_name": series_name,
                "video_path": video_path,
                "release_date": release_date,
            }
        )

    return series_groups


def process_segments_for_episode(
    series_name: str,
    episode_id: str,
    video_path: str,
    duration_s: float,
    release_date: Any,
) -> List[Dict[str, Any]]:
    """è™•ç†å–®é›†çš„ç‰‡æ®µç´šåˆ¥æŸ¥è©¢ç”Ÿæˆ"""
    segment_ranges = []
    start = 0.0
    while start < duration_s:
        end = min(start + SEGMENT_LENGTH, duration_s)
        if end - start < 5:
            break
        segment_ranges.append((start, end))
        if end >= duration_s:
            break
        start = start + SEGMENT_LENGTH - SEGMENT_OVERLAP

    segment_files = []
    for seg_idx, (s, e) in enumerate(
        tqdm(segment_ranges, desc=f"åˆ‡å‰²ç‰‡æ®µ {episode_id}", unit="seg")
    ):
        # ä½¿ç”¨ series_name é¿å…æª”åè¡çª
        safe_series = series_name.replace(" ", "_").replace("/", "_")
        segment_video_path = CACHE_DIR / f"segment_{safe_series}_{episode_id}_seg{seg_idx}.mp4"
        if not segment_video_path.exists():
            extract_video_segment(video_path, s, e, segment_video_path)

        segment_files.append(
            {
                "index": seg_idx,
                "path": segment_video_path,
            }
        )

    seg_results = []
    for seg_info in segment_files:
        seg_idx = seg_info["index"]
        segment_path = seg_info["path"]

        safe_series = series_name.replace(" ", "_").replace("/", "_")
        cache_path = CACHE_DIR / f"segment_{safe_series}_{episode_id}_seg{seg_idx}.json"
        if cache_path.exists():
            # å°±ç®—æœ‰ cacheï¼Œä¹Ÿå¹«å®ƒè£œä¸Š series_name / release_dateï¼Œé¿å…èˆŠæª”æ˜¯ç©ºçš„
            with open(cache_path, "r", encoding="utf-8") as f:
                cached = json.load(f)
            cached["series_name"] = series_name
            cached["release_date"] = release_date
            if "file_name" not in cached:
                cached["file_name"] = f"videos/segment_{episode_id}_seg{seg_idx}.mp4"
            seg_results.append(cached)
            # å›å¯«ä¸€æ¬¡ï¼Œè®“æª”æ¡ˆä¹Ÿè®Šæˆæ–°çš„
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(cached, f, ensure_ascii=False, indent=2)
            continue

        # ä½¿ç”¨ retry åŒ…è£…ä¸Šä¼ å’ŒæŸ¥è¯¢ç”Ÿæˆ
        def process_segment():
            client = get_client()
            file_uri = upload_video_to_gemini(client, str(segment_path))
            return generate_segment_queries(client=client, file_uri=file_uri)
        
        try:
            data = call_with_retry(process_segment)
        except BlockedContentError as e:
            print(f"âš ï¸  ç‰‡æ®µ {seg_idx} è¢«é˜»æ­¢ï¼Œè·³é: {e}")
            # å‰µå»ºä¸€å€‹ç©ºçš„æŸ¥è©¢è¨˜éŒ„
            data = {
                "visual_saliency": ["å…§å®¹è¢«é˜»æ­¢"] * 3,
                "character_emotion": ["å…§å®¹è¢«é˜»æ­¢"] * 3,
                "action_behavior": ["å…§å®¹è¢«é˜»æ­¢"] * 3,
                "dialogue": ["å…§å®¹è¢«é˜»æ­¢"] * 3,
                "symbolic_scene": ["å…§å®¹è¢«é˜»æ­¢"] * 3,
            }

        record = {
            "series_name": series_name,
            "episode_id": episode_id,
            "segment_index": seg_idx,
            "release_date": release_date,
            "file_name": f"videos/{safe_series}/segment_{safe_series}_{episode_id}_seg{seg_idx}.mp4",
            "query": data,  # æ”¹åç‚º query
        }

        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(record, f, ensure_ascii=False, indent=2)

        seg_results.append(record)

        # ä¸Šå‚³å½±ç‰‡åˆ° HF
        upload_video_to_hf(
            HF_REPO_SEGMENT,
            segment_path,
            record["file_name"]
        )

    return seg_results


def process_episode_level(
    series_name: str,
    episode_id: str,
    video_path: str,
    duration_s: float,
    release_date: Any,
) -> Dict[str, Any]:
    """è™•ç†é›†æ•¸ç´šåˆ¥çš„æŸ¥è©¢ç”Ÿæˆ"""
    
    # ä½¿ç”¨ retry åŒ…è£…ä¸Šä¼ å’ŒæŸ¥è¯¢ç”Ÿæˆ
    def process_episode():
        client = get_client()
        file_uri = upload_video_to_gemini(client, str(video_path))
        return generate_episode_queries(client=client, file_uri=file_uri)
    
    epi_result = call_with_retry(process_episode)

    # ä¸Šå‚³å®Œæ•´é›†æ•¸å½±ç‰‡åˆ° HF
    safe_series = series_name.replace(" ", "_").replace("/", "_")
    episode_video_hf_path = f"videos/{safe_series}/episode_{safe_series}_{episode_id}.mp4"
    print("  ğŸ“¤ ä¸Šå‚³å®Œæ•´é›†æ•¸åˆ° HuggingFace...")
    upload_video_to_hf(HF_REPO_EPISODE, Path(video_path), episode_video_hf_path)

    # åŒ…è£ episode metadata èˆ‡æ¨¡å‹å›æ‡‰
    episode_record = {
        "file_name": episode_video_hf_path,  # æ·»åŠ  file_name å­—æ®µç”¨æ–¼ data viewer
        "series_name": series_name,
        "episode_id": episode_id,  # æ”¹åç‚º episode_id
        "release_date": release_date,
        "query": epi_result,  # æ”¹åç‚º query
    }

    # ä»ç„¶ä¿å­˜å–®å€‹ JSON æ–‡ä»¶ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
    epi_local = CACHE_DIR / f"episode_{safe_series}_{episode_id}.json"
    with open(epi_local, "w", encoding="utf-8") as f:
        json.dump(episode_record, f, ensure_ascii=False, indent=2)
    update_episode_metadata(HF_TOKEN)

    return episode_record


def process_single_episode(
    series_name: str, episode_info: Dict[str, Any]
) -> Tuple[str, str, float, Any, Dict[str, Any], List[Dict[str, Any]]]:
    episode_id = episode_info["episode_id"]
    video_path = episode_info["video_path"]
    release_date = episode_info.get("release_date")
    duration_s = get_video_duration_from_path(video_path)

    safe_series = series_name.replace(" ", "_").replace("/", "_")
    epi_local = CACHE_DIR / f"episode_{safe_series}_{episode_id}.json"
    seg_local = CACHE_DIR / f"segment_{safe_series}_{episode_id}.json"

    if epi_local.exists() and seg_local.exists():
        with open(epi_local, "r", encoding="utf-8") as f:
            episode_record = json.load(f)
        with open(seg_local, "r", encoding="utf-8") as f:
            seg_results = json.load(f)
        
        return (
            episode_id,
            video_path,
            duration_s,
            release_date,
            episode_record,
            seg_results,
        )

    print(f"\n{'='*60}")
    print(f"è™•ç†é›†æ•¸: {episode_id}")
    print(f"å½±ç‰‡é•·åº¦: {duration_s:.2f} ç§’")
    print(f"{'='*60}\n")

    # ===== Segment level =====
    seg_results = process_segments_for_episode(
        series_name,
        episode_id,
        video_path,
        duration_s,
        release_date,
    )

    safe_series = series_name.replace(" ", "_").replace("/", "_")
    seg_local = CACHE_DIR / f"segment_{safe_series}_{episode_id}.json"
    with open(seg_local, "w", encoding="utf-8") as f:
        json.dump(seg_results, f, ensure_ascii=False, indent=2)
    update_segment_metadata(HF_TOKEN)

    # ===== Episode level =====
    episode_record = process_episode_level(
        series_name,
        episode_id,
        video_path,
        duration_s,
        release_date,
    )

    return (
        episode_id,
        video_path,
        duration_s,
        release_date,
        episode_record,
        seg_results,
    )



def process_series_level(
    series_name: str, processed_episodes: List[Tuple[str, str, float, Any]]
) -> Dict[str, Any]:
    """è™•ç†ç³»åˆ—ç´šåˆ¥çš„æŸ¥è©¢ç”Ÿæˆ"""

    safe_series = series_name.replace(" ", "_").replace("/", "_")
    series_local = CACHE_DIR / f"series_{safe_series}.json"
    if series_local.exists():
        print(f"âœ… series {series_name} å·²æœ‰å¿«å–ï¼Œç•¥éé‡æ–°ç”Ÿæˆ")
        with open(series_local, "r", encoding="utf-8") as f:
            series_record = json.load(f)
        return series_record
    
    # åˆä½µä¸¦ä¸Šå‚³æ•´å­£å½±ç‰‡
    print("  æº–å‚™æ•´å­£å½±ç‰‡...")
    episode_video_paths = [vp for _, vp, _, _ in processed_episodes]
    safe_series = series_name.replace(" ", "_").replace("/", "_")
    series_video_path = CACHE_DIR / f"series_{safe_series}.mp4"
    if not series_video_path.exists():
        print("  ğŸ”— é–‹å§‹åˆä½µæ•´å­£å½±ç‰‡...")

        tmp_list = series_video_path.with_suffix(".txt")
        with open(tmp_list, "w", encoding="utf-8") as f:
            for p in episode_video_paths:
                f.write(f"file '{Path(p).absolute()}'\n")
        cmd = [
            "ffmpeg",
            "-f", "concat",
            "-safe", "0",
            "-i", str(tmp_list),
            "-c", "copy",
            str(series_video_path),
        ]
        subprocess.run(cmd, check=True)
        tmp_list.unlink(missing_ok=True)

    low_fps_series_video_path = CACHE_DIR / f"series_{safe_series}_low_fps.mp4"
    if not low_fps_series_video_path.exists():
        down_video_fps(series_video_path, low_fps_series_video_path)

    # ä½¿ç”¨ retry åŒ…è£…ä¸Šä¼ å’ŒæŸ¥è¯¢ç”Ÿæˆ
    def process_series():
        client = get_client()
        file_uri = upload_video_to_gemini(client, str(low_fps_series_video_path))
        return generate_series_queries(client=client, file_uri=file_uri)
    
    series_result = call_with_retry(process_series)

    print("  ğŸ“¤ ä¸Šå‚³æ•´å­£å½±ç‰‡åˆ° HuggingFace...")
    upload_video_to_hf(
        HF_REPO_SERIES, series_video_path, f"videos/series_{safe_series}.mp4"
    )

    # å»ºç«‹ series metadataï¼ˆåƒ…ä¿ç•™å¿…è¦æ¬„ä½ï¼‰
    release_dates = sorted(
        {rd for (_eid, _vp, _dur, rd) in processed_episodes if rd is not None}
    )
    series_record = {
        "file_name": f"videos/series_{safe_series}.mp4",  # æ·»åŠ  file_name å­—æ®µç”¨æ–¼ data viewer
        "series_name": series_name,
        "release_date": release_dates[0] if release_dates else None,  # ä½¿ç”¨é¦–æ’­æ—¥æœŸ
        "query": series_result,  # æ”¹åç‚º query
    }

    with open(series_local, "w", encoding="utf-8") as f:
        json.dump(series_record, f, ensure_ascii=False, indent=2)
    update_series_metadata(HF_TOKEN)

    return series_record


def process_single_series(series_name: str, episodes: List[Dict[str, Any]]) -> Tuple[
    Tuple[str, List[Tuple[str, str, float, Any]]],
    List[Dict[str, Any]],
    List[Dict[str, Any]],
    Dict[str, Any],
]:
    """è™•ç†å–®å€‹ç³»åˆ—çš„æ‰€æœ‰é›†æ•¸ï¼Œè¿”å› (result, episode_metadata, segment_metadata, series_metadata)"""
    print(f"é–‹å§‹è™•ç†ç³»åˆ—: {series_name} (å…± {len(episodes)} é›†)")

    processed_episodes = []
    episode_metadata = []
    segment_metadata = []

    for episode_info in tqdm(episodes, desc=f"episodes - {series_name}", unit="ep"):
        episode_id, video_path, duration_s, release_date, episode_record, seg_results = (
            process_single_episode(series_name, episode_info)
        )
        processed_episodes.append((episode_id, video_path, duration_s, release_date))
        episode_metadata.append(episode_record)
        segment_metadata.extend(seg_results)

    # è™•ç†ç³»åˆ—ç´šåˆ¥
    series_record = process_series_level(series_name, processed_episodes)

    return (
        (series_name, processed_episodes),
        episode_metadata,
        segment_metadata,
        series_record,
    )


def process_all_series(
    series_groups: Dict[str, List[Dict[str, Any]]],
) -> Tuple[
    List[Tuple[str, List[Tuple[str, str, float, Any]]]],
    List[Dict[str, Any]],
    List[Dict[str, Any]],
    List[Dict[str, Any]],
]:
    """è™•ç†æ‰€æœ‰ç³»åˆ—ï¼Œè¿”å› (processed_series, all_episode_metadata, all_segment_metadata, all_series_metadata)"""
    processed_series = []
    all_episode_metadata = []
    all_segment_metadata = []
    all_series_metadata = []

    for series_name, episodes in series_groups.items():
        result, episode_metadata, segment_metadata, series_metadata = (
            process_single_series(series_name, episodes)
        )
        processed_series.append(result)
        all_episode_metadata.extend(episode_metadata)
        all_segment_metadata.extend(segment_metadata)
        all_series_metadata.append(series_metadata)

    return (
        processed_series,
        all_episode_metadata,
        all_segment_metadata,
        all_series_metadata,
    )


# ================== ä¸»ç¨‹å¼ ==================
def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    ensure_hf_repos()

    print(f"å·²è¨­å®š {len(GEMINI_API_KEYS)} å€‹ Gemini API Key")

    # è¼‰å…¥ä¸¦åˆ†çµ„è³‡æ–™é›†
    series_groups = load_and_group_dataset()
    series_groups = {k: series_groups[k] for k in list(series_groups)[:5]}

    # è™•ç†æ‰€æœ‰ç³»åˆ—èˆ‡ episodes
    (
        processed_series,
        all_episode_metadata,
        all_segment_metadata,
        all_series_metadata,
    ) = process_all_series(series_groups)

    # å‰µå»º metadata.jsonl æ–‡ä»¶ä»¥å•Ÿç”¨ Dataset Viewer
    if all_episode_metadata:
        create_metadata_jsonl(HF_REPO_EPISODE, all_episode_metadata, "metadata.jsonl")

    if all_segment_metadata:
        create_metadata_jsonl(HF_REPO_SEGMENT, all_segment_metadata, "metadata.jsonl")

    if all_series_metadata:
        create_metadata_jsonl(HF_REPO_SERIES, all_series_metadata, "metadata.jsonl")

    # å®Œæˆç¸½çµ
    total_series = len(processed_series)
    total_episodes = sum(len(eps) for _, eps in processed_series)

    print("\n" + "=" * 60)
    print("ğŸ‰ è™•ç†å®Œæˆï¼")
    print(f"âœ… è™•ç†äº† {total_series} å€‹ç³»åˆ—ï¼Œ{total_episodes} é›†")
    print(f"ğŸ“Š Episode metadata: {len(all_episode_metadata)} ç­†")
    print(f"ğŸ“Š Segment metadata: {len(all_segment_metadata)} ç­†")
    print("ğŸ” Dataset Viewer ç¾å·²å•Ÿç”¨ï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
