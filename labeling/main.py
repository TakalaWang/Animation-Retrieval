import os
import json
import time
import logging
import re
from pathlib import Path
from typing import Any, Dict, List

from tqdm import tqdm, trange
from dotenv import load_dotenv
from datasets import load_dataset
from huggingface_hub import HfApi, create_repo

import google.genai as genai
from moviepy import VideoFileClip, concatenate_videoclips



# ä½ è‡ªå·±çš„æ¨¡çµ„
from episode_processor import process_episode
from series_processor import process_series

# ================== åŸºæœ¬è¨­å®š ==================
logging.basicConfig(level=logging.INFO)
load_dotenv()

# æ”¯æ´å–®å€‹æˆ–å¤šå€‹ API Keyï¼ˆç”¨é€—è™Ÿåˆ†éš”æˆ– JSON é™£åˆ—ï¼‰
GEMINI_API_KEYS = [k.strip() for k in os.getenv("GEMINI_API_KEY", "").strip().split(',') if k.strip()]

HF_TOKEN = os.getenv("HF_TOKEN", "")

if not GEMINI_API_KEYS:
    raise RuntimeError("è«‹å…ˆè¨­å®š GEMINI_API_KEY")
if not HF_TOKEN:
    raise RuntimeError("è«‹å…ˆè¨­å®š HF_TOKEN")

# API Key è¼ªæ›
current_key_index = 0

def get_next_api_key():
    """è¼ªæ›ä½¿ç”¨ API Key"""
    global current_key_index
    key = GEMINI_API_KEYS[current_key_index]
    current_key_index = (current_key_index + 1) % len(GEMINI_API_KEYS)
    return key

HF_REPO_SEGMENT = "TakalaWang/anime-2024-segment-queries"
HF_REPO_EPISODE = "TakalaWang/anime-2024-episode-queries"
HF_REPO_SERIES = "TakalaWang/anime-2024-series-queries"

CACHE_DIR = Path("./cache_gemini_video")
CACHE_DIR.mkdir(parents=True, exist_ok=True)

TEST_DATASET = "JacobLinCool/anime-2024"
TEST_SPLIT = "winter"
SEGMENT_LENGTH = 60
SEGMENT_OVERLAP = 5

def get_client():
    """ç²å– Gemini clientï¼ˆä½¿ç”¨è¼ªæ›çš„ API keyï¼‰"""
    return genai.Client(api_key=get_next_api_key())


# ================== HF å·¥å…· ==================
def ensure_hf_repos():
    create_repo(HF_REPO_SEGMENT, token=HF_TOKEN, repo_type="dataset", exist_ok=True)
    create_repo(HF_REPO_EPISODE, token=HF_TOKEN, repo_type="dataset", exist_ok=True)
    create_repo(HF_REPO_SERIES, token=HF_TOKEN, repo_type="dataset", exist_ok=True)


def upload_json_to_hf(repo_id: str, path: Path, repo_path: str):
    api = HfApi(token=HF_TOKEN)
    api.upload_file(
        path_or_fileobj=str(path),
        repo_id=repo_id,
        path_in_repo=repo_path,
        repo_type="dataset",
    )


def upload_video_to_hf(repo_id: str, video_path: Path, repo_path: str):
    """ä¸Šå‚³å½±ç‰‡æª”æ¡ˆåˆ° HuggingFace"""
    api = HfApi(token=HF_TOKEN)
    api.upload_file(
        path_or_fileobj=str(video_path),
        repo_id=repo_id,
        path_in_repo=repo_path,
        repo_type="dataset",
    )
    print(f"ğŸ“¤ å·²ä¸Šå‚³å½±ç‰‡åˆ° HF: {repo_path}")


def extract_video_segment(video_path: str, start_s: float, end_s: float, output_path: Path):
    """åˆ‡å‰²å½±ç‰‡ç‰‡æ®µ"""
    with VideoFileClip(video_path) as video:
        segment = video.subclipped(start_s, end_s)
        segment.write_videofile(
            str(output_path),
            codec="libx264",
            audio_codec="aac",
            temp_audiofile=str(output_path.parent / f"temp_{output_path.stem}_audio.m4a"),
            remove_temp=True,
            logger=None,  # æ¸›å°‘è¼¸å‡º
        )
    print(f"âœ‚ï¸  å·²åˆ‡å‰²ç‰‡æ®µ: {output_path.name}")


def concatenate_videos(video_paths: List[str], output_path: Path):
    """åˆä½µå¤šå€‹å½±ç‰‡"""
    clips = []
    for path in video_paths:
        clips.append(VideoFileClip(path))
    
    final_clip = concatenate_videoclips(clips, method="compose")
    final_clip.write_videofile(
        str(output_path),
        codec="libx264",
        audio_codec="aac",
        temp_audiofile=str(output_path.parent / f"temp_{output_path.stem}_audio.m4a"),
        remove_temp=True,
        logger=None,
    )
    
    # é—œé–‰æ‰€æœ‰ clips
    for clip in clips:
        clip.close()
    final_clip.close()
    
    print(f"ğŸ”— å·²åˆä½µå½±ç‰‡: {output_path.name}")


# ================== åªæœ‰ Gemini ç”¨çš„ retry ==================
def call_with_retry(fn, *args, **kwargs):
    sleep_sec = kwargs.pop("sleep_sec", 5)
    while True:
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            msg = str(e)
            if "429" in msg or "rate" in msg.lower():
                print(f"[rate limited] sleep {sleep_sec}s and retry ...")
                time.sleep(sleep_sec)
                continue
            # ä¸æ˜¯ rate limit ä¹Ÿä¸€æ¨£ç­‰ä¸€ä¸‹å†è©¦
            print(f"[error] {e} -> sleep {sleep_sec}s and retry ...")
            time.sleep(sleep_sec)


# ================== æœ¬åœ°å½±ç‰‡é•·åº¦ ==================
def get_video_duration_from_path(path: str) -> float:
    with VideoFileClip(path) as clip:
        return clip.duration  # ç§’ç‚ºå–®ä½ (float)

# ================== ä¸Šå‚³æœ¬åœ°æª”åˆ° Gemini ==================
def upload_file_if_local(file_uri: str, client: genai.Client) -> str:
    """
    - http/https/s3/gs â†’ ç›´æ¥å›å‚³
    - æœ¬åœ° â†’ ä¸Šå‚³ã€ç­‰ ACTIVEã€å›å‚³ uri
    """
    if re.match(r"^(https?://|s3://|gs://)", file_uri):
        return file_uri
    
    if file_uri.startswith("file://"):
        file_uri = file_uri[len("file://"):]
    
    p = Path(file_uri).expanduser()
    if not p.exists():
        raise FileNotFoundError(p)

    print(f"â¬†ï¸  ä¸Šå‚³åˆ° Gemini: {p.name}", flush=True)
    resp = client.files.upload(file=str(p))
    
    # è¼ªè©¢åˆ° ACTIVE
    while resp.state.name == "PROCESSING":
        print("â³ è™•ç†ä¸­...", flush=True)
        time.sleep(10)
        resp = client.files.get(name=resp.name)

    if resp.state.name != "ACTIVE":
        raise RuntimeError(f"æª”æ¡ˆè™•ç†å¤±æ•—: {resp.state}")

    print(f"âœ… ä¸Šå‚³å®Œæˆ: {resp.uri}", flush=True)
    return resp.uri


# ================== ä¸»ç¨‹å¼ ==================
def main():
    ensure_hf_repos()
    
    print(f"ğŸ“ å·²è¨­å®š {len(GEMINI_API_KEYS)} å€‹ Gemini API Key")

    print("è¼‰å…¥è³‡æ–™é›†...")
    ds = load_dataset(TEST_DATASET, TEST_SPLIT, split="train")
    ds_raw = ds.with_format("arrow")

    # ä¾ series åˆ†çµ„
    series_groups: Dict[str, List[Dict[str, Any]]] = {}
    for i in trange(len(ds_raw), desc="group by series"):
        row = ds_raw[i:i+1]
        series_name = row["series_name"][0].as_py()
        episode_name = row["episode_name"][0].as_py()
        video_path = row["video"][0]["path"].as_py()
        duration = row["duration"][0].as_py() if "duration" in row.column_names else None

        series_groups.setdefault(series_name, []).append({
            "episode_name": episode_name,
            "series_name": series_name,
            "video_path": video_path,
            "duration": duration,
        })

    # ç¾åœ¨å…ˆè™•ç†ç¬¬ä¸€å€‹ series
    first_series = list(series_groups.keys())[0]
    episodes = series_groups[first_series]

    print(f"é–‹å§‹è™•ç†ç³»åˆ—: {first_series} (å…± {len(episodes)} é›†)")
    print("ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šåªè™•ç†ç¬¬ä¸€é›†")

    # åªå–ç¬¬ä¸€é›†ä¾†æ¸¬è©¦
    episodes = episodes[:1]

    processed_episodes = []
    episode_video_paths = []  # ç”¨æ–¼æœ€å¾Œåˆä½µæ•´å­£

    for idx, ex in enumerate(tqdm(episodes, desc="episodes", unit="ep"), 1):
        episode_id = ex["episode_name"] or f"{first_series}_ep{idx:02d}"
        video_path = ex["video_path"]

        # é•·åº¦
        duration_s = float(ex["duration"]) if ex.get("duration") else get_video_duration_from_path(video_path)

        print(f"\n{'='*60}")
        print(f"è™•ç†é›†æ•¸: {episode_id}")
        print(f"å½±ç‰‡é•·åº¦: {duration_s:.2f} ç§’")
        print(f"{'='*60}\n")

        # ===== Segment level =====
        print("ğŸ“ æ­¥é©Ÿ 1: åˆ‡å‰²å½±ç‰‡ç‰‡æ®µ...")
        
        # å…ˆåœ¨æœ¬åœ°åˆ‡å‰²æ‰€æœ‰ç‰‡æ®µ
        segment_files = []
        start = 0.0
        seg_idx = 0
        
        while start < duration_s:
            end = min(start + SEGMENT_LENGTH, duration_s)
            
            # å¦‚æœå‰©é¤˜æ™‚é–“å¤ªçŸ­ï¼ˆå°æ–¼ 5 ç§’ï¼‰ï¼Œå°±ä½µå…¥ä¸Šä¸€å€‹ç‰‡æ®µæˆ–è·³é
            if end - start < 5:
                break
            
            segment_video_path = CACHE_DIR / f"segment_{episode_id}_seg{seg_idx}.mp4"
            
            if not segment_video_path.exists():
                print(f"  âœ‚ï¸  åˆ‡å‰²ç‰‡æ®µ {seg_idx}: {start:.1f}s - {end:.1f}s")
                extract_video_segment(video_path, start, end, segment_video_path)
            else:
                print(f"  ğŸ“¦ ä½¿ç”¨å¿«å–ç‰‡æ®µ {seg_idx}: {segment_video_path.name}")
            
            segment_files.append({
                "index": seg_idx,
                "start_s": start,
                "end_s": end,
                "path": segment_video_path,
            })
            
            # å¦‚æœé€™å€‹ç‰‡æ®µå·²ç¶“åˆ°é”å½±ç‰‡çµå°¾ï¼ŒçµæŸå¾ªç’°
            if end >= duration_s:
                break
            
            # è¨ˆç®—ä¸‹ä¸€å€‹ç‰‡æ®µçš„èµ·å§‹ä½ç½®ï¼ˆæœ‰é‡ç–Šï¼‰
            start = start + SEGMENT_LENGTH - SEGMENT_OVERLAP
            seg_idx += 1

        print(f"\nğŸ“ æ­¥é©Ÿ 2: ä¸Šå‚³ {len(segment_files)} å€‹ç‰‡æ®µåˆ° Gemini ä¸¦ç”ŸæˆæŸ¥è©¢...")
        
        # è™•ç†æ¯å€‹ç‰‡æ®µï¼šä¸Šå‚³åˆ° Gemini -> ç”ŸæˆæŸ¥è©¢ -> ä¸Šå‚³åˆ° HF
        seg_results = []
        for seg_info in segment_files:
            seg_idx = seg_info["index"]
            segment_path = seg_info["path"]
            start_s = seg_info["start_s"]
            end_s = seg_info["end_s"]
            
            # æª¢æŸ¥å¿«å–
            cache_path = CACHE_DIR / f"segment_{episode_id}_seg{seg_idx}.json"
            if cache_path.exists():
                with open(cache_path, "r", encoding="utf-8") as f:
                    cached = json.load(f)
                    seg_results.append(cached)
                    print(f"  ğŸ“¦ ä½¿ç”¨å¿«å–æŸ¥è©¢: ç‰‡æ®µ {seg_idx}")
                    continue
            
            # ä¸Šå‚³ç‰‡æ®µåˆ° Gemini
            print(f"  â¬†ï¸  ä¸Šå‚³ç‰‡æ®µ {seg_idx} åˆ° Gemini...")
            client = get_client()
            segment_uri = upload_file_if_local(str(segment_path), client)
            
            # ç”ŸæˆæŸ¥è©¢
            print(f"  ğŸ¬ ç”ŸæˆæŸ¥è©¢: ç‰‡æ®µ {seg_idx}")
            from segment_processor import generate_segment_queries
            
            data = call_with_retry(
                generate_segment_queries,
                client=client,
                file_uri=segment_uri,
                sleep_sec=5,
            )
            
            record = {
                "episode_id": episode_id,
                "segment_index": seg_idx,
                "start_s": start_s,
                "end_s": end_s,
                "queries": data,
            }
            
            # å„²å­˜å¿«å–
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(record, f, ensure_ascii=False, indent=2)
            
            seg_results.append(record)
            
            # ä¸Šå‚³ç‰‡æ®µå½±ç‰‡åˆ° HF
            print(f"  ğŸ“¤ ä¸Šå‚³ç‰‡æ®µ {seg_idx} åˆ° HuggingFace...")
            upload_video_to_hf(
                HF_REPO_SEGMENT,
                segment_path,
                f"videos/{first_series}/segment_{episode_id}_seg{seg_idx}.mp4"
            )
        
        # ä¸Šå‚³ segment JSON å½™ç¸½
        seg_local = CACHE_DIR / f"segment_{episode_id}.json"
        with open(seg_local, "w", encoding="utf-8") as f:
            json.dump(seg_results, f, ensure_ascii=False, indent=2)
        upload_json_to_hf(HF_REPO_SEGMENT, seg_local, f"segment_{episode_id}.json")

        # ===== Episode level =====
        print(f"\nğŸ“ æ­¥é©Ÿ 3: è™•ç†å®Œæ•´é›†æ•¸...")
        
        # ä¸Šå‚³å®Œæ•´å½±ç‰‡åˆ° Gemini
        print("  â¬†ï¸  ä¸Šå‚³å®Œæ•´é›†æ•¸åˆ° Gemini...")
        client = get_client()
        uploaded_uri = upload_file_if_local(video_path, client)
        
        epi_result = process_episode(
            client=client,
            episode_id=episode_id,
            file_uri=uploaded_uri,
            cache_dir=CACHE_DIR,
            retry_fn=call_with_retry,
        )
        
        # ä¸Šå‚³å®Œæ•´é›†æ•¸å½±ç‰‡åˆ° HF
        episode_video_hf_path = f"videos/{first_series}/episode_{episode_id}.mp4"
        print("  ğŸ“¤ ä¸Šå‚³å®Œæ•´é›†æ•¸åˆ° HuggingFace...")
        upload_video_to_hf(HF_REPO_EPISODE, Path(video_path), episode_video_hf_path)
        
        # ä¸Šå‚³ episode JSON
        epi_local = CACHE_DIR / f"episode_{episode_id}.json"
        with open(epi_local, "w", encoding="utf-8") as f:
            json.dump(epi_result, f, ensure_ascii=False, indent=2)
        upload_json_to_hf(HF_REPO_EPISODE, epi_local, f"episode_{episode_id}.json")

        processed_episodes.append((episode_id, uploaded_uri, duration_s))
        episode_video_paths.append(video_path)

    # ===== Series level =====
    print(f"\nğŸ“ æ­¥é©Ÿ 4: è™•ç†æ•´å­£è³‡æ–™...")
    
    client = get_client()
    series_result = process_series(
        client=client,
        series_id=first_series,
        episodes=processed_episodes,
        cache_dir=CACHE_DIR,
        retry_fn=call_with_retry,
    )
    
    # åˆä½µä¸¦ä¸Šå‚³æ•´å­£å½±ç‰‡ï¼ˆæ¸¬è©¦æ¨¡å¼ä¸‹åªæœ‰ä¸€é›†ï¼Œæ‰€ä»¥ç›´æ¥è¤‡è£½ï¼‰
    print("  ï¿½ æº–å‚™æ•´å­£å½±ç‰‡...")
    series_video_path = CACHE_DIR / f"series_{first_series}.mp4"
    if not series_video_path.exists():
        if len(episode_video_paths) == 1:
            # åªæœ‰ä¸€é›†ï¼Œç›´æ¥è¤‡è£½
            import shutil
            shutil.copy2(episode_video_paths[0], series_video_path)
            print(f"  ğŸ“‹ å·²è¤‡è£½å½±ç‰‡ä½œç‚ºæ•´å­£: {series_video_path.name}")
        else:
            # å¤šé›†ï¼Œéœ€è¦åˆä½µ
            print("  ğŸ”— é–‹å§‹åˆä½µæ•´å­£å½±ç‰‡...")
            concatenate_videos(episode_video_paths, series_video_path)
    
    print("  ğŸ“¤ ä¸Šå‚³æ•´å­£å½±ç‰‡åˆ° HuggingFace...")
    upload_video_to_hf(
        HF_REPO_SERIES,
        series_video_path,
        f"videos/series_{first_series}.mp4"
    )
    
    # ä¸Šå‚³ series JSON
    series_local = CACHE_DIR / f"series_{first_series}.json"
    with open(series_local, "w", encoding="utf-8") as f:
        json.dump(series_result, f, ensure_ascii=False, indent=2)
    upload_json_to_hf(HF_REPO_SERIES, series_local, f"series_{first_series}.json")

    print("\n" + "="*60)
    print("ğŸ‰ æ¸¬è©¦å®Œæˆï¼")
    print(f"âœ… è™•ç†äº† {len(episodes)} é›†")
    print(f"âœ… ç”Ÿæˆäº† {len(seg_results)} å€‹ç‰‡æ®µ")
    print("="*60)


if __name__ == "__main__":
    main()
